<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>phd on</title><link>https://jadenlorenc.com/tags/phd/</link><description>Recent content in phd on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://jadenlorenc.com/tags/phd/index.xml" rel="self" type="application/rss+xml"/><item><title>dr fulda call notes feb 2024</title><link>https://jadenlorenc.com/dr-fulda-call-notes-feb-2024/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/dr-fulda-call-notes-feb-2024/</guid><description>#hebbian #phd
2024-02-13 12:54
consider a custom dataset, specifically built to prioritize learning new info fast. It would have a defined sequence, with related information grouped together in time.</description></item><item><title>evolve_and_merge</title><link>https://jadenlorenc.com/evolve_and_merge/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/evolve_and_merge/</guid><description>#hebbian #phd
2024-02-12 07:13
[[List of Hebbian Weight Update Rules#ABCD|ABCD]], again. Just the same as [[zotero_notes/hebbian/meta-learning-through-hebbian-plasticity-KG2BPTBB#^5c0f88|meta-learning-through-hebbian-plasticity-KG2BPTBB]] The point in this one was to reduce the number of rules while also increasing generalizability.</description></item><item><title>evolving neuromodulatory topologies</title><link>https://jadenlorenc.com/evolving-neuromodulatory-topologies/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/evolving-neuromodulatory-topologies/</guid><description>#hebbian #phd
2024-02-12 07:19
[[List of Hebbian Weight Update Rules#ABCD|ABCD]] once more, this time with genetically decided topology. Topology, in this case, meaning number of neurons and connections.</description></item><item><title>FA_Ex</title><link>https://jadenlorenc.com/FA_Ex/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/FA_Ex/</guid><description>#hebbian #phd
2024-02-10 16:57
Honestly just seems like it&amp;rsquo;s a fancy way to say, &amp;ldquo;Multiply the hebbian rule by your error signal&amp;rdquo;.</description></item><item><title>HPCA</title><link>https://jadenlorenc.com/HPCA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/HPCA/</guid><description>#hebbian #phd
[[2024-02-07]]
An extension to [[Ojas Rule]], pulling the principle of extracting the main direction from the data, out into the space for more than just one neuron at a time.</description></item><item><title>HPCA Deets</title><link>https://jadenlorenc.com/HPCA-Deets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/HPCA-Deets/</guid><description>#hebbian #phd
[[2024-02-07]]
An extension to [[Ojas Rule]], pulling the principle of extracting the main direction from the data, out into the space for more than just one neuron at a time.</description></item><item><title>Ojas Rule</title><link>https://jadenlorenc.com/Ojas-Rule/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Ojas-Rule/</guid><description>#phd #hebbian
[[2024-02-07]]
An addition to the basic [[List of Hebbian Weight Update Rules#Simple Hebbian Learning Rule|hebbian]] learning rule that adds in the y to the subtracted term.</description></item><item><title>Synaptic Balancing</title><link>https://jadenlorenc.com/Synaptic-Balancing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Synaptic-Balancing/</guid><description>#hebbian #phd
2024-02-12 14:19
This was a very confusing and math-heavy paper.
chat transcript Certainly! Let&amp;rsquo;s outline a simple algorithm for updating synaptic weights in a neural network using a synaptic balancing rule, along with the computation and tracking of average gain.</description></item></channel></rss>