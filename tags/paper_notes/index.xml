<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>paper_notes on</title><link>https://jadenlorenc.com/tags/paper_notes/</link><description>Recent content in paper_notes on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://jadenlorenc.com/tags/paper_notes/index.xml" rel="self" type="application/rss+xml"/><item><title>COPER Paper</title><link>https://jadenlorenc.com/COPER-Paper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/COPER-Paper/</guid><description>#work/patientsim #paper_notes [[2024-02-05]]
It&amp;rsquo;s really similar to [[HybridML Paper|hybridml]], on account of the way that it pulls in mechanistic models (in this case Neural Ordinary Differential Equations).</description></item><item><title>coupled rnn paper</title><link>https://jadenlorenc.com/coupled-rnn-paper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/coupled-rnn-paper/</guid><description>#work/patientsim #paper_notes
[[2024-02-05]]
One RNN gives its output to the input of the other. Each rnn gets a different overall input.</description></item><item><title>Generating Synthetic Data Paper</title><link>https://jadenlorenc.com/Generating-Synthetic-Data-Paper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Generating-Synthetic-Data-Paper/</guid><description>#paper_notes #work/patientsim
Generating Synthetic Data Paper I took like 6hrs to crack this paper, reading it off and on. Since the authors were mainly interested in generating patient data, the encoding of the data into a good latent space was treated as a given.</description></item><item><title>HybridML Paper</title><link>https://jadenlorenc.com/HybridML-Paper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/HybridML-Paper/</guid><description>#work/patientsim #paper_notes
[[2024-02-05]]
Not much, just a way to combine mechanistic models with ML, using easy json and TensorFlow. Bizarrely, they claim to be an open platform whilst paywalling the paper.</description></item><item><title>Informer paper</title><link>https://jadenlorenc.com/Informer-paper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Informer-paper/</guid><description>#work/patientsim #paper_notes [[2024-02-05]]
[2012.07436] Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting published in 2021
It&amp;rsquo;s just a [[transformer|transformer]] with $O(L \log{L})$ complexity in its attention mechanism, using some sparse version of self-attention via distilling it.</description></item><item><title>Merkelbach Paper</title><link>https://jadenlorenc.com/Merkelbach-Paper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Merkelbach-Paper/</guid><description>#work/patientsim #paper_notes
Merkelbach version Based on the MIMIC dataset.
data prep time series data collected, normalized filled in missing data, imputed via linear interpolation, or in the case of no intag:#work/patientsimstance of this data at all, the median static data tacked onto every single instance of the time series data GRU (it&amp;rsquo;s bidirectional) processes the whole of the data in both directions.</description></item><item><title>meta-learning with hospital data</title><link>https://jadenlorenc.com/meta-learning-with-hospital-data/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/meta-learning-with-hospital-data/</guid><description>#work/patientsim #paper_notes [[2024-02-05]]
DynEHR, written about in 2021, was frustrating to figure out, due to a paywall. I just grabbed its description from a paper that speaks about it:</description></item><item><title>PatchMixer</title><link>https://jadenlorenc.com/PatchMixer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/PatchMixer/</guid><description>#work/patientsim #paper_notes [[2024-02-05]]
A convolutional neural network!! Haven&amp;rsquo;t seen one in a minute, and certainly didn&amp;rsquo;t expect it in time series processing.</description></item><item><title>Patient Similarity Readings Log</title><link>https://jadenlorenc.com/Patient-Similarity-Readings-Log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Patient-Similarity-Readings-Log/</guid><description>#work/patientsim
1 2 3 list file.title + dateformat(file.ctime, &amp;#34; yy-MM-dd&amp;#34;) + &amp;#34; - &amp;#34; + file.tags from #work/patientsim and #paper_notes sort file.</description></item><item><title>triplet transformer for patient data</title><link>https://jadenlorenc.com/triplet-transformer-for-patient-data/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/triplet-transformer-for-patient-data/</guid><description>#work/patientsim #paper_notes
Self-Supervised Transformer Seems like it&amp;rsquo;s the modern, best way to do it. Triplets are stored, solving the whole sparsity problem in both the time and feature space.</description></item></channel></rss>