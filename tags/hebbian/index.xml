<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>hebbian on</title><link>https://jadenlorenc.com/tags/hebbian/</link><description>Recent content in hebbian on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://jadenlorenc.com/tags/hebbian/index.xml" rel="self" type="application/rss+xml"/><item><title>Candidate Weight Changes</title><link>https://jadenlorenc.com/Candidate-Weight-Changes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Candidate-Weight-Changes/</guid><description>#hebbian #ideas
[[2024-01-30]]
My loss graph looks wild:
![[/files/W&amp;amp;BChart1_30_202485501AM.png|600]]
Weights &amp;amp; Biases
I attribute the wave pattern to the self-reinforcing [[Hebbian Learning|weight updates]] improving performance quickly when they&amp;rsquo;ve found something right, then rapidly diverging because of biases inherent in that pattern being magnified and getting out of control.</description></item><item><title>Comparing the performance of Hebbian against backpropagation learning using conv nns</title><link>https://jadenlorenc.com/Omnivore/2023-08-04/Comparing-the-performance-of-Hebbian-against-backpropagation-learning-using-conv-nns/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Omnivore/2023-08-04/Comparing-the-performance-of-Hebbian-against-backpropagation-learning-using-conv-nns/</guid><description>Comparing the performance of Hebbian against backpropagation learning using conv nns #Omnivore #hebbian
Read on Omnivore
Read Original</description></item><item><title>Comparing the performance of Hebbian against backpropagation learning using convolutional neural ...</title><link>https://jadenlorenc.com/Omnivore/2023-08-04/Comparing-the-performance-of-Hebbian-against-backpropagation-learning-using-convolutional-neural-.../</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Omnivore/2023-08-04/Comparing-the-performance-of-Hebbian-against-backpropagation-learning-using-convolutional-neural-.../</guid><description>Comparing the performance of Hebbian against backpropagation learning using convolutional neural networks | SpringerLink #Omnivore #hebbian
Read on Omnivore Read Original</description></item><item><title>Cyclic Plasticity Values</title><link>https://jadenlorenc.com/Cyclic-Plasticity-Values/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Cyclic-Plasticity-Values/</guid><description>#hebbian #ideas
Inspired by a youtube video by Artem Kirsanov (I think it might be this one), which describes how biological neurons have a 6hr period of varying plasticity, and each might be most plastic at a different time than the rest.</description></item><item><title>decay chat transcript</title><link>https://jadenlorenc.com/decay-chat-transcript/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/decay-chat-transcript/</guid><description>#hebbian
Me i wanna deep dive into just #2, the weight decay. Grab the relevant passage, repeat it back to me, then break it down.</description></item><item><title>deep dive into weight decay term</title><link>https://jadenlorenc.com/deep-dive-into-weight-decay-term/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/deep-dive-into-weight-decay-term/</guid><description>#hebbian
the term: ![[List of Hebbian Weight Update Rules#Weight Decay Term Added]]
The problem with [the basic Hebbian learning rule] is that there are no mechanisms to prevent the weights from growing unbounded, thus leading to possible instability.</description></item><item><title>dr fulda call notes feb 2024</title><link>https://jadenlorenc.com/dr-fulda-call-notes-feb-2024/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/dr-fulda-call-notes-feb-2024/</guid><description>#hebbian #phd
2024-02-13 12:54
consider a custom dataset, specifically built to prioritize learning new info fast. It would have a defined sequence, with related information grouped together in time.</description></item><item><title>Evaluating Hebbian Learning in a Semi-supervised Setting - SpringerLink</title><link>https://jadenlorenc.com/Omnivore/2023-08-04/Evaluating-Hebbian-Learning-in-a-Semi-supervised-Setting-SpringerLink/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Omnivore/2023-08-04/Evaluating-Hebbian-Learning-in-a-Semi-supervised-Setting-SpringerLink/</guid><description>Evaluating Hebbian Learning in a Semi-supervised Setting | SpringerLink #Omnivore #hebbian
Read on Omnivore
Read Original
Highlights In this work, we propose a semi-supervised learning approach, in which the unsupervised pre-training step is performed by means of the Hebbian learning paradigm.</description></item><item><title>evolve_and_merge</title><link>https://jadenlorenc.com/evolve_and_merge/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/evolve_and_merge/</guid><description>#hebbian #phd
2024-02-12 07:13
[[List of Hebbian Weight Update Rules#ABCD|ABCD]], again. Just the same as [[zotero_notes/hebbian/meta-learning-through-hebbian-plasticity-KG2BPTBB#^5c0f88|meta-learning-through-hebbian-plasticity-KG2BPTBB]] The point in this one was to reduce the number of rules while also increasing generalizability.</description></item><item><title>evolving neuromodulatory topologies</title><link>https://jadenlorenc.com/evolving-neuromodulatory-topologies/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/evolving-neuromodulatory-topologies/</guid><description>#hebbian #phd
2024-02-12 07:19
[[List of Hebbian Weight Update Rules#ABCD|ABCD]] once more, this time with genetically decided topology. Topology, in this case, meaning number of neurons and connections.</description></item><item><title>FA_Ex</title><link>https://jadenlorenc.com/FA_Ex/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/FA_Ex/</guid><description>#hebbian #phd
2024-02-10 16:57
Honestly just seems like it&amp;rsquo;s a fancy way to say, &amp;ldquo;Multiply the hebbian rule by your error signal&amp;rdquo;.</description></item><item><title>Hebbian Learning</title><link>https://jadenlorenc.com/Hebbian-Learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Hebbian-Learning/</guid><description>#hebbian
It&amp;rsquo;s an alternative to backprop that uses only local update rules. It seems far more similar to spiking neural networks in that it tries to follow a more &amp;ldquo;fires together, wires together&amp;rdquo; paradigm.</description></item><item><title>HPCA</title><link>https://jadenlorenc.com/HPCA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/HPCA/</guid><description>#hebbian #phd
[[2024-02-07]]
An extension to [[Ojas Rule]], pulling the principle of extracting the main direction from the data, out into the space for more than just one neuron at a time.</description></item><item><title>HPCA Deets</title><link>https://jadenlorenc.com/HPCA-Deets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/HPCA-Deets/</guid><description>#hebbian #phd
[[2024-02-07]]
An extension to [[Ojas Rule]], pulling the principle of extracting the main direction from the data, out into the space for more than just one neuron at a time.</description></item><item><title>I Built a Hebbian RNN next steps</title><link>https://jadenlorenc.com/I-Built-a-Hebbian-RNN-next-steps/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/I-Built-a-Hebbian-RNN-next-steps/</guid><description>#hebbian
2024-02-09 08:06
Four things to work on next:
Try out some new rules found in the survey, [[List of Hebbian Weight Update Rules|listed here]] Use a more optimized dataset, perhaps simpler.</description></item><item><title>Layer Normalization</title><link>https://jadenlorenc.com/Layer-Normalization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Layer-Normalization/</guid><description>#hebbian
The technique stabilizes hidden state dynamics in RNNs. It may be an alternative solution to some previous issues I had with my [[Hebbian Learning|hebbian]] experiments, where the weight values, activations, and subsequently the loss numbers got increasingly erratic and large over time.</description></item><item><title>List of Hebbian Weight Update Rules</title><link>https://jadenlorenc.com/List-of-Hebbian-Weight-Update-Rules/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/List-of-Hebbian-Weight-Update-Rules/</guid><description>#hebbian
Simple Hebbian Learning Rule: $$ \Delta w_i = \eta y x_i $$
Weight Decay Term Added: $$ \Delta w_i = \eta y x_i - \gamma(\mathbf{w}, \mathbf{x}) $$</description></item><item><title>Ojas Rule</title><link>https://jadenlorenc.com/Ojas-Rule/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Ojas-Rule/</guid><description>#phd #hebbian
[[2024-02-07]]
An addition to the basic [[List of Hebbian Weight Update Rules#Simple Hebbian Learning Rule|hebbian]] learning rule that adds in the y to the subtracted term.</description></item><item><title>Synaptic Balancing</title><link>https://jadenlorenc.com/Synaptic-Balancing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Synaptic-Balancing/</guid><description>#hebbian #phd
2024-02-12 14:19
This was a very confusing and math-heavy paper.
chat transcript Certainly! Let&amp;rsquo;s outline a simple algorithm for updating synaptic weights in a neural network using a synaptic balancing rule, along with the computation and tracking of average gain.</description></item></channel></rss>