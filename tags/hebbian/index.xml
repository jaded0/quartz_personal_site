<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>hebbian on</title><link>https://jadenlorenc.com/tags/hebbian/</link><description>Recent content in hebbian on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://jadenlorenc.com/tags/hebbian/index.xml" rel="self" type="application/rss+xml"/><item><title>Candidate Weight Changes</title><link>https://jadenlorenc.com/Candidate-Weight-Changes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Candidate-Weight-Changes/</guid><description>#hebbian #ideas
[[2024-01-30]]
My loss graph looks wild:
![[/files/W&amp;amp;BChart1_30_202485501AM.png|600]]
Weights &amp;amp; Biases
I attribute the wave pattern to the self-reinforcing [[Hebbian Learning|weight updates]] improving performance quickly when they&amp;rsquo;ve found something right, then rapidly diverging because of biases inherent in that pattern being magnified and getting out of control.</description></item><item><title>Comparing the performance of Hebbian against backpropagation learning using conv nns</title><link>https://jadenlorenc.com/Omnivore/2023-08-04/Comparing-the-performance-of-Hebbian-against-backpropagation-learning-using-conv-nns/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Omnivore/2023-08-04/Comparing-the-performance-of-Hebbian-against-backpropagation-learning-using-conv-nns/</guid><description>Comparing the performance of Hebbian against backpropagation learning using conv nns #Omnivore #hebbian
Read on Omnivore
Read Original</description></item><item><title>Comparing the performance of Hebbian against backpropagation learning using convolutional neural ...</title><link>https://jadenlorenc.com/Omnivore/2023-08-04/Comparing-the-performance-of-Hebbian-against-backpropagation-learning-using-convolutional-neural-.../</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Omnivore/2023-08-04/Comparing-the-performance-of-Hebbian-against-backpropagation-learning-using-convolutional-neural-.../</guid><description>Comparing the performance of Hebbian against backpropagation learning using convolutional neural networks | SpringerLink #Omnivore #hebbian
Read on Omnivore Read Original</description></item><item><title>Cyclic Plasticity Values</title><link>https://jadenlorenc.com/Cyclic-Plasticity-Values/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Cyclic-Plasticity-Values/</guid><description>#hebbian #ideas
Inspired by a youtube video by Artem Kirsanov (I think it might be this one), which describes how biological neurons have a 6hr period of varying plasticity, and each might be most plastic at a different time than the rest.</description></item><item><title>decay chat transcript</title><link>https://jadenlorenc.com/decay-chat-transcript/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/decay-chat-transcript/</guid><description>#hebbian
Me i wanna deep dive into just #2, the weight decay. Grab the relevant passage, repeat it back to me, then break it down.</description></item><item><title>deep dive into weight decay term</title><link>https://jadenlorenc.com/deep-dive-into-weight-decay-term/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/deep-dive-into-weight-decay-term/</guid><description>#hebbian
the term: ![[List of Hebbian Weight Update Rules#Weight Decay Term Added]]
The problem with [the basic Hebbian learning rule] is that there are no mechanisms to prevent the weights from growing unbounded, thus leading to possible instability.</description></item><item><title>Evaluating Hebbian Learning in a Semi-supervised Setting - SpringerLink</title><link>https://jadenlorenc.com/Omnivore/2023-08-04/Evaluating-Hebbian-Learning-in-a-Semi-supervised-Setting-SpringerLink/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Omnivore/2023-08-04/Evaluating-Hebbian-Learning-in-a-Semi-supervised-Setting-SpringerLink/</guid><description>Evaluating Hebbian Learning in a Semi-supervised Setting | SpringerLink #Omnivore #hebbian
Read on Omnivore
Read Original
Highlights In this work, we propose a semi-supervised learning approach, in which the unsupervised pre-training step is performed by means of the Hebbian learning paradigm.</description></item><item><title>Hebbian Learning</title><link>https://jadenlorenc.com/Hebbian-Learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Hebbian-Learning/</guid><description>#hebbian
It&amp;rsquo;s an alternative to backprop that uses only local update rules. It seems far more similar to spiking neural networks in that it tries to follow a more &amp;ldquo;fires together, wires together&amp;rdquo; paradigm.</description></item><item><title>HPCA Deets</title><link>https://jadenlorenc.com/HPCA-Deets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/HPCA-Deets/</guid><description>#hebbian #phd
[[2024-02-07]]
An extension to [[Ojas Rule]], pulling the principle of extracting the main direction from the data, out into the space for more than just one neuron at a time.</description></item><item><title>Layer Normalization</title><link>https://jadenlorenc.com/Layer-Normalization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Layer-Normalization/</guid><description>#hebbian
The technique stabilizes hidden state dynamics in RNNs. It may be an alternative solution to some previous issues I had with my [[Hebbian Learning|hebbian]] experiments, where the weight values, activations, and subsequently the loss numbers got increasingly erratic and large over time.</description></item><item><title>List of Hebbian Weight Update Rules</title><link>https://jadenlorenc.com/List-of-Hebbian-Weight-Update-Rules/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/List-of-Hebbian-Weight-Update-Rules/</guid><description>#hebbian
Simple Hebbian Learning Rule: $$ \Delta w_i = \eta y x_i $$
Weight Decay Term Added: $$ \Delta w_i = \eta y x_i - \gamma(\mathbf{w}, \mathbf{x}) $$</description></item><item><title>Ojas Rule</title><link>https://jadenlorenc.com/Ojas-Rule/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Ojas-Rule/</guid><description>#phd #hebbian
[[2024-02-07]]
An addition to the basic [[List of Hebbian Weight Update Rules#Simple Hebbian Learning Rule|hebbian]] learning rule that adds in the y to the subtracted term.</description></item></channel></rss>