{"/":{"title":"🪴 jaden lorenc","content":"\n\nI work with the door open. I'll update this site with every little project and research tangent, hopefully giving those who want to work with me a sort of live transcript to go off of. \n\nPlease talk to Jaden-Bot! It's on the bottom-right corner, and knows a lot about me. \nIf you want to work with me, call or text: (385) 327-2503\n## current status\n- Choosing grad schools to study deep learning. \n- Researching [hebbian](Hebbian%20Learning.md) gradient update rules. I'm building an RNN that uses [[List of Hebbian Weight Update Rules|local learning rules]], hoping that, as an emergent behavior, it will encode medium-term \"memories\" within the weights themselves. \n- Check out the [report](https://wandb.ai/jadens_team/hebby/reports/prog--Vmlldzo2ODQxMzQ0) that benchmarks various local weight update rules. If you have any idea why they all seem to be converging to that arbitrary value, please email me!\n- Researching [[patient record model type breakdown|patient trajectory embeddings]] for use in case control studies at a children's hospital. I get paid for this one!\n\n### side projects\n- Slowly building up [Convostarter](https://www.convostarter.app/), an app for language learning. \n\n### backburner\n- Revisiting an old experment; I wanted to [augment WLASL](Augment%20WLASL.md) -- a word-level American Sign Language dataset -- by running it through some stable diffusion models, perhaps guided by ControlNet. I have some proximity to the Deaf community, so I may be able to get some feedback on the results.\n- Considering some sort of [[Visualize a Codebase]] system, allowing me to more quickly understand what's going on in open source projects. \n\n### finished projects\n- Just built a chatbot on this site to allow people to interview \"me\" on-demand.\n- An internship building document understanding for life science methods documents.\n## goals\nI really just want to work on new AI models.","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":[]},"/Alt-Transformer-Project-Description":{"title":"Alt Transformer Project Description","content":"#project \nI'm a driven Data Scientist Intern, with a deep interest in machine learning and natural language processing. I recently finished a course in Deep Learning and have worked on impactful projects such as drug discovery research with a professor. My skills extend beyond Python and Git, including the rapid cross-platform development tool, Flutter. My passion aligns closely with the latest developments in machine learning, ensuring I'm always updated with current trends. Throughout my academic journey, I've worked on several projects like building parsers, investigating into RoseTTAFold algorithm, and leading a startup for aspiring authors. My most recent and personally exciting projects are:\n- A chatbot for my personal site that allows interviewer to interview \"me\" on demand.\n- A finetuned video diffusion model for word-level sign language generation.\n- A small variation of a transformer model that attempts to abstract-away the attention operation down to a single matrix multiplication between the activations of two layers. It loses the ability to vary the context length, but I feel that the increased flexibility might actually create some advantages. It seems to train competitively to GPT-2, but it may lose competitive advantage due to its lack of implicit bias towards preserving long-term dependencies.","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["project"]},"/Augment-WLASL":{"title":"Augment WLASL","content":"#stablediffusion  \n# previous attempt\nMy little attempt at straight training an entire stable diffusion model on just the WLASL dataset was fairly ineffectual. Months of training on university compute resources culminated in a bit of a blurry mess:\n![[/files/example_book_43402_7bbb996a33a6757f0c3d.gif|500]]\n\nThis is supposed to be the sign for 'book'.\n\nI spent probably about 100 hours, total, trying to force this to work, fiddling with hyperparameters and diving into the code to learn what might be improved. I tried my own set of augmentations: cropping the windows randomly, skipping frames, cropping the video at different times. \nThe odyssey, with loss graphs and sample outputs, is logged [here](https://wandb.ai/jadens_team/vid-signs?workspace=user)].\n\nTruly, I needed an existing video diffusion model, but there wasn't one available at the time.\n\n# current go at it\n- There are now some pretty solid video diffusion models. \n- ControlNet-guided diffusion does a pretty good job at maintaining some temporal consistency between frames. \n- Temporal conditioning techniques also exist, allowing me to pass frames into existing stablediffusion models and expect better consistency between them. \n- I now have access to GPT-4, which offers a few advantages:\n\t- The large model may possess enough contextual knowledge of ASL to offer some data augmentation avenues as far as the labeling of the videos go.\n\t\t- I can add in words that are signed the same to the labels. \n\t\t- I can attempt to disambiguate some words.\n\t- I can work towards a full translation pipeline. I believe that GPT-4 is likely able to translate spoken english to an intelligible ASL representation using only word-level knowledge.\n\t\t- I could potentially use this to transform a person's photo into a live ASL translating avatar.","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["stablediffusion"]},"/COPER-Paper":{"title":"COPER Paper","content":" #work/patientsim #paper_notes\n[[2024-02-05]]\n\nIt's really similar to [[HybridML Paper|hybridml]], on account of the way that it pulls in mechanistic models (in this case Neural Ordinary Differential Equations). It then just uses a [[Perceiver Paper|perceiver]].\n\n[COPER: Continuous Patient State Perceiver](https://ar5iv.labs.arxiv.org/html/2208.03196)\n","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["paper_notes"]},"/Candidate-Weight-Changes":{"title":"Candidate Weight Changes","content":" #hebbian #ideas \n\n[[2024-01-30]]\n\nMy loss graph looks wild:\n\n![[/files/W\u0026BChart1_30_202485501AM.png|600]]\n\n\n[Weights \u0026 Biases](https://wandb.ai/jadens_team/hebby/runs/qmezepr7?workspace=user-jaden-lorenc)\n\nI attribute the wave pattern to the self-reinforcing [[Hebbian Learning|weight updates]] improving performance quickly when they've found something right, then rapidly diverging because of biases inherent in that pattern being magnified and getting out of control. Unfortunately, it's like it's rewarding extreme-ness too much.\n\nMaybe my training pattern is wavy because the training process doesn't actually check if a change is effective before making an edit to the weights. A solution could be to create another set of parameters which represent candidate changes, and my imprint only adds to it. Those candidate parameters, with the same shape as the weights of the network, are just added onto them during a run when activations are calculated. Crucially, they're not modifying the weights, yet. Only after we have a reward signal based on the weights+candidate do we add the candidate to the value of the weights as a long-term change, modulated by the reward. Then, we wipe the candidate parameter. \n\nAlternatively, we could just divide the candidate parameter by two or something. When the imprint (activationXactivation) is added to it, the candidate weight change gets a sort of long tail of evaluation. \n\nThe benefit of all this is that a weight change is only applied if it's actually improving the reward. The long tail ought to only help that by requiring it to improve the reward over many iterations.\n","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["hebbian","ideas"]},"/Cyclic-Plasticity-Values":{"title":"Cyclic Plasticity Values","content":"\n #hebbian #ideas \n\nInspired by a youtube video by Artem Kirsanov (I think it might be [this one](https://youtu.be/iV-EMA5g288?si=RJ7I36tyjFiU4YC2)), which describes how biological neurons have a 6hr period of varying plasticity, and each might be most plastic at a different time than the rest. It's thought to drive sparsity, and distributed representations in the brain. \n\nAnother parameter for each weight, determining its plasticity, a value between 0 and 1 that multiplies with any weight update directed toward the weight. Perhaps there's a set of parameters that just determine different aspects of the sine wave, like so:\n\n$$ y = A \\sin(B(x - C)) + D $$\n\nWhere:\n- $A$ is the **Amplitude**, affecting vertical stretching.\n- $B$ influences the **Frequency**, impacting horizontal stretching.\n- $C$ is the **Phase Shift**, moving the wave horizontally.\n- $D$ represents the **Vertical Shift**, moving the wave vertically.\n\nThese parameters could likely be meta-learned, like in [[zotero_notes/hebbian/meta-learning-through-hebbian-plasticity-KG2BPTBB|meta-learning-through-hebbian-plasticity]]. Actually, the idea in that paper closely resembles this one. ","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["hebbian","ideas"]},"/Denoising-Diffusion-Probabilistic-Models-Notes":{"title":"Denoising Diffusion Probabilistic Models Notes","content":"\n#cs/advdl \n\n![[Pasted image 20230216095306.png]]\nforward and backward processes are reversed in this picture. The forward proces is actually from right to left.\n\n\n# algorithms\n$q\\left(\\mathbf{x}_0\\right)$  is just the sample from the data distribution.\nalpha hat t is really close to 1, and we just multiply it with the image in step 5\ne is the noise, so it's small. \nOur network being trained is trying to predict the *noise* instead of the clean image. So when $t$ is big, the network is almost an identity function, just let through the noise. When $t$ is small, it has to eke out the noise that it has. \nDouble bars is L2 norm. \n$$\n\\begin{aligned}\n\u0026 \\hline \\text { Algorithm } 1 \\text { Training } \\\\\n\u0026 \\hline \\text { 1: } \\quad \\text { repeat } \\\\\n\u0026 \\text { 2: } \\quad \\mathbf{x}_0 \\sim q\\left(\\mathbf{x}_0\\right) \\\\\n\u0026 \\text { 3: } \\quad t \\sim \\text { Uniform }(\\{1, \\ldots, T\\}) \\\\\n\u0026 \\text { 4: } \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\\\\n\u0026 \\text { 5: } \\quad \\text { Take gradient descent step on } \\\\\n\u0026 \\quad \\quad \\nabla_\\theta\\left\\|\\boldsymbol{\\epsilon}-\\boldsymbol{\\epsilon}_\\theta\\left(\\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0+\\sqrt{1-\\bar{\\alpha}_t} \\boldsymbol{\\epsilon}, t\\right)\\right\\|^2 \\\\\n\u0026 \\text { 6: until converged }\n\\end{aligned}\n$$\n\n\n$z$ is gaussian, and turns to zero at penultimate step of diffusion.\n4: subtracting off noise, adding other extra noise.\n$$\n\\begin{aligned}\n\u0026 \\text { Algorithm } 2 \\text { Sampling } \\\\\n\u0026 \\hline \\text { 1: } \\mathbf{x}_T \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\\\\n\u0026 \\text { 2: for } t=T, \\ldots, 1 \\text { do } \\\\\n\u0026 \\text { 3: } \\quad \\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\text { if } t\u003e1, \\text { else } \\mathbf{z}=\\mathbf{0} \\\\\n\u0026 \\text { 4: } \\quad \\mathbf{x}_{t-1}=\\frac{1}{\\sqrt{\\alpha_t}}\\left(\\mathbf{x}_t-\\frac{1-\\alpha t}{\\sqrt{1-\\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_\\theta\\left(\\mathbf{x}_t, t\\right)\\right)+\\sigma_t \\mathbf{z} \\\\\n\u0026 \\text { 5: end for } \\\\\n\u0026 \\text { 6: return } \\mathbf{x}_0\n\\end{aligned}\n$$\n\n# crazy vocab\n\n\u003e optimizing the usual variational bound on negative log likelihood\n\n[[Denoising Diffusion Probabilistic Models#^i4lyzrh1sui|right here]]\n???\n## jensen's Inequality\nIf our hard to optimize function is always more than an easier to optimize function, you can use the easier to optimize function. As long as you're trying to maximize it. \nIt's a convex/concave thing. Your inequality will always hold if what you need to be convex is convex. \n\n# markov assumption\n[markov assumption](https://en.wikipedia.org/wiki/Causal_Markov_condition)\n$p(a,b,c,d)=p(a)\\ p(b|a)\\ p(c|a,b)\\ p(d|a,b,c)$\nyou can change it up:\n$= p(a)\\ p(c|d)\\ p(c,d)\\ p(a|b,c,d)$\nso also: $p(b|a)=\\frac{p(b)\\ p(a|b)}{p(a)}$ \nThe **markov assumption** is where you take away the complicated dependencies, making all dependencies a chain. So under the markov assumption, $p(a,b,c,d) = p(a)\\ p(b|a)\\ p(c|b)\\ p(d|c)$, instead. \n\n[[Denoising Diffusion Probabilistic Models#^i4lyzrh1sui|this]] is done because of the markov assumption. \n\n[[Denoising Diffusion Probabilistic Models#^xw39thwc9|equation 1]]: $q\\left(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}\\right):=\\mathcal{N}\\left(\\mathbf{x}_t ; \\sqrt{1-\\beta_t} \\mathbf{x}_{t-1}, \\beta_t \\mathbf{I}\\right)$ \u003e objective is to learn mu thing and sigma thing.\n\n# getting gaussian\nat any t for time, we can get the gaussian for the noise. that's kinda neat.\n\n# equation 5 \nfirst little guy just drops out. the second term added is important.\nclosed form??\n[[Denoising Diffusion Probabilistic Models#^3z3x8ipbs9r|equation 5]]\n$\\mathbb{E}_q[\\underbrace{D_{\\mathrm{KL}}\\left(q\\left(\\mathbf{x}_T \\mid \\mathbf{x}_0\\right) \\| p\\left(\\mathbf{x}_T\\right)\\right)}_{L_T}+\\sum_{t\u003e1} \\underbrace{D_{\\mathrm{KL}}\\left(q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0\\right) \\| p_\\theta\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t\\right)\\right)}_{L_{t-1}} \\underbrace{-\\log p_\\theta\\left(\\mathbf{x}_0 \\mid \\mathbf{x}_1\\right)}_{L_0}]$\n# chatgpt questions\n\nin the following equation, why condition on $x_0$? Training is performed by optimizing the usual variational bound on negative log likelihood: $$ \\mathbb{E}\\left[-\\log p_\\theta\\left(\\mathbf{x}_0\\right)\\right] \\leq \\mathbb{E}_q\\left[-\\log \\frac{p_\\theta\\left(\\mathbf{x}_{0: T}\\right)}{q\\left(\\mathbf{x}_{1: T} \\mid \\mathbf{x}_0\\right)}\\right]=\\mathbb{E}_q\\left[-\\log p\\left(\\mathbf{x}_T\\right)-\\sum_{t \\geq 1} \\log \\frac{p_\\theta\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t\\right)}{q\\left(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}\\right)}\\right]=: L $$\n\nThe equation conditions on $x_0$ because it represents the initial observation in a time series. The goal of this equation is to optimize the negative log likelihood of the first observation $x_0$ under the probabilistic model $p_\\theta$. However, because the model's probability distribution depends on previous observations, the optimization cannot be done directly on $x_0$.","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":[]},"/FA_Ex":{"title":"FA_Ex","content":" #hebbian #phd  \n2024-02-10 16:57\n\nHonestly just seems like it's a fancy way to say, \"Multiply the hebbian rule by your error signal\". It was bringing [[partial derivative and backprop|backprop]] and [[Omnivore/2023-08-11/Feedback alignment in deep convolutional net- works|Feedback alignment]] into it, idk. \nThe mathematical notation is weak in this paper. It's super unclear whether the output is used in the weights update or if it's the *derivative* of the output. It may actually, if the latter case is true, be more like [[List of Hebbian Weight Update Rules#Differential Hebbian Learning (DHL)|differential hebbian learning]] with a global training/reward signal tacked on. \n\nHere's a diagram of what they essentially are doing:\n![[zotero_notes/hebbian/FA_Ex-J5ETYZL4#^5db1d8|FA_Ex-J5ETYZL4]]\n\n[[zotero_notes/hebbian/FA_Ex-J5ETYZL4#^9aa788|the caption]]:\nSchematic illustration of three learning rules:\n1. **BP (backpropagation):** Requires the information in $W_2$ to backpropagate.\n2. **FA (feedback alignment):** Requires heterogeneity in the tentative impact of the middle layer neurons on the output.\n3. **FA_Ex-100% (feedback alignment with 100% excitatory neurons in middle layer):** Most biologically plausible, as it computes using locally available information at a synaptic triad. Its performance is fairly good and comparable to BP.\n\nWith the notations:\n- $y_i^{(1)} := f\\left( \\sum_j W_{ij}^{(1)} x_j \\right)$\n- $y^{(2)} := f\\left( \\sum_i W_i^{(2)} y_i^{(1)} \\right)$\n- $J := \\frac{e^2}{2} = \\frac{(y^{(2)} - y)^2}{2}$\n\nThe gradient for BP is given by:\n$$ \\frac{\\partial J}{\\partial W_{ij}^{(1)}} = e W_i^{(2)} f'\\left( \\sum_j W_{ij}^{(1)} x_j \\right) \\cdot x_j $$\n\nIn FA, $W_i^{(2)}$ is replaced by a random number $B_i$, and in FA_Ex-100%, it's replaced by 1. Therefore, for FA_Ex-100%, the synaptic weights in the middle layer are updated by the following rule:\n$$ (\\Delta W)_{ij} = -0.01 \\frac{\\partial J}{\\partial W_{ij}^{(1)}} = -0.01 x_j \\theta(I_i) e $$\n\nHere, $\\theta$ is a step function and $I_i$ is the current input to neuron $i$. This can be interpreted as:\n$$ (\\Delta W)_{ij} \\propto \\text{pre}_i \\times \\text{post}_j \\times \\text{dopamine} $$\n\nInterestingly, simplified and biologically plausible learning rules like FA_Ex-100% work robustly as long as the error signal, possibly conveyed by dopaminergic neurons, is accurate.\n\n# chat transcript\n### Original FA_Ex-100% Learning Rule\nThe original FA_Ex-100% rule is given by:\n$$ (1W)_{ij} = -0.01 \\frac{\\partial J}{\\partial W_{ij}^1} = -0.01 x_j \\theta(I_i) e $$\nwhere:\n- $(1W)_{ij}$: Change in weight for the synapse from neuron $j$ to neuron $i$.\n- $-0.01$: Learning rate.\n- $x_j$: Pre-synaptic activity (input to the synapse).\n- $\\theta(I_i)$: Step function of the input to neuron $i$, representing post-synaptic activity.\n- $e$: Error signal, related to the difference between actual and desired output.\n\n### Revised FA_Ex-100% Learning Rule using Standard Notation for Error Signal\nThe revised FA_Ex-100% rule using standard error signal notation is:\n$$ \\Delta W_{ij} = \\eta \\cdot x_j \\cdot \\theta(I_i) \\cdot \\frac{\\partial L}{\\partial y_i} $$\nwhere:\n- $\\Delta W_{ij}$: Change in weight for the synapse from neuron $j$ to neuron $i$.\n- $\\eta$: Learning rate.\n- $x_j$: Pre-synaptic activity.\n- $\\theta(I_i)$: Step function of the input to neuron $i$, representing post-synaptic activity.\n- $\\frac{\\partial L}{\\partial y_i}$: Error signal, represented as the derivative of the loss function with respect to the output of neuron $i$.\n","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["hebbian","phd","Differential"]},"/Generating-Synthetic-Data-Paper":{"title":"Generating Synthetic Data Paper","content":"#paper_notes  #work/patientsim \n# Generating Synthetic Data Paper\nI took like 6hrs to crack this paper, reading it off and on. Since the authors were mainly interested in generating patient data, the encoding of the data into a good latent space was treated as a given. I initially thought I had cracked it, thinking that they simply threw a concatenated set of time blocks and their features into a normal VAE.\n![[zotero_notes/patient_sim/fakedata-DJMSSG8H#notes]]\n\nNay, upon looking into its code, I find LSTMs inside the VAEs. The [[zotero_notes/patient_sim/fakedata-supplement-AXDGBHD7#fakedata supplement|supplementary info]] explains that they were just basing their idea on the 2015 paper called [DRAW](https://arxiv.org/pdf/1502.04623.pdf).\n","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["paper_notes","fakedata"]},"/HPCA":{"title":"HPCA","content":"#hebbian #phd  \n[[2024-02-07]]\n\nAn extension to [[Ojas Rule]], pulling the principle of extracting the main direction from the data, out into the space for more than just one neuron at a time. Basically performing a sort of principal component analysis among all neurons in the layer. \n[[PCA]] is a similar concept, basically wanting to find the main concepts that represent the original data. HPCA is basically asking the weight vector to change in such a way that improves the whole layer's ability to reconstruct the inputs if it were needed. \n\nHere's the equation:\n![[List of Hebbian Weight Update Rules#HPCA]]\n\nAnd the accompanying Lagani paper intuition:\n![[zotero_notes/hebbian/hebbian-semi-supervised-in-sample-efficent-annotes-ECAMC5NF#^85af19|hebbian-semi-supervised-in-sample-efficent-annotes-ECAMC5NF]]\n\n[[zotero_notes/hebbian/Survey-of-Hebbian-Learning-Lagani-2023-CF4RA22D#^0c21e9|more Lagani intuition]]\n\nIt's pretty competitive vs [[Variational Autoencoder|VAEs]] and [[partial derivative and backprop|backprop]], when the availability of labeled samples is below 10% (I assume of the available samples).\n\nPooling layers are [[zotero_notes/hebbian/hebbian-semi-supervised-in-sample-efficent-annotes-ECAMC5NF#^a1899a|crucial]] to its performance, interestingly enough.\n# chat breakdown\n\nThe HPCA (Hebbian Principal Component Analysis) learning rule is given by:\n\n$$\n\\Delta \\mathbf{w}_i = \\eta y_i \\left( \\mathbf{x} - \\sum_{j=1}^{i} y_j \\mathbf{w}_j \\right)\n$$\n\nIn this rule:\n\n- $\\Delta \\mathbf{w}_i$ is the change in the weight vector of the $i^{th}$ neuron.\n- $\\eta$ is the learning rate.\n- $y_i$ is the output of the $i^{th}$ neuron.\n- $\\mathbf{x}$ is the input vector.\n- $\\sum_{j=1}^{i} y_j \\mathbf{w}_j$ is the subtracted summation term, which represents the reconstruction of the input vector up to the $i^{th}$ component.\n\nIn traditional Hebbian learning, the rule is often expressed as \"cells that fire together wire together,\" implying that the synaptic weight is increased when both the presynaptic and postsynaptic neurons are active. A common extension to this rule includes a decay term to prevent unbounded growth of the weights.\n\nIn HPCA, the subtracted summation term $\\sum_{j=1}^{i} y_j \\mathbf{w}_j$ effectively replaces the decay term, but it does more than just control the growth of the weights. This term ensures that each weight vector learns to represent different principal components of the input data:\n\n1. **Orthogonality and Decorrelation**: The term $\\mathbf{x} - \\sum_{j=1}^{i} y_j \\mathbf{w}_j$ represents the residual error after removing the contributions of all the previous components (from 1 to $i$) from the input $\\mathbf{x}$. This ensures that each weight vector $\\mathbf{w}_i$ learns to be orthogonal to the previous weight vectors $\\mathbf{w}_1, \\mathbf{w}_2, ..., \\mathbf{w}_{i-1}$. Orthogonality is a key property of the principal components in PCA, where each component is uncorrelated with the others.\n\n2. **Variance Maximization**: By projecting the input $\\mathbf{x}$ onto the residual error, the algorithm ensures that each weight vector aligns with the direction of maximum variance in the residual. This is akin to how each principal component in PCA captures the direction of maximum variance in the data that is not already captured by the previous components.\n\n3. **Incremental Learning**: Unlike batch PCA, which computes all principal components at once using the entire dataset, HPCA learns the components incrementally. This allows it to be applied in online or streaming data scenarios, adapting to new data as it becomes available.\n\nThus, the subtracted summation in the HPCA learning rule effectively guides each neuron in the network to learn different principal components of the input data, following the spirit of PCA in terms of finding orthogonal directions of maximum variance, but in a neural network context and with an incremental, online learning capability.\n\n\n# crazy idea\n#ideas \nWhat if instead of making it reconstruct the current input, we make it reconstruct the *next* input?? Store weights and activations, and then use this update rule, using new $\\mathbf{x}$, old $y_j$ and old $\\mathbf{w}_j$. It's like an automatic self-supervision, or something. idk about $y_i$, though.","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["hebbian","phd","ideas"]},"/HPCA-Deets":{"title":"HPCA Deets","content":"#hebbian #phd  \n[[2024-02-07]]\n\nAn extension to [[Ojas Rule]], pulling the principle of extracting the main direction from the data, out into the space for more than just one neuron at a time. Basically performing a sort of principal component analysis among all neurons in the layer. \n[[PCA]] is a similar concept, basically wanting to find the main concepts that represent the original data. HPCA is basically asking the weight vector to change in such a way that improves the whole layer's ability to reconstruct the inputs if it were needed. \n\nHere's the equation:\n![[List of Hebbian Weight Update Rules#HPCA]]\n\nAnd the accompanying Lagani paper intuition:\n![[zotero_notes/hebbian/hebbian-semi-supervised-in-sample-efficent-annotes-ECAMC5NF#^85af19|hebbian-semi-supervised-in-sample-efficent-annotes-ECAMC5NF]]\n\n[[zotero_notes/hebbian/Survey-of-Hebbian-Learning-Lagani-2023-CF4RA22D#^0c21e9|more Lagani intuition]]\n\nIt's pretty competitive vs [[Variational Autoencoder|VAEs]] and [[partial derivative and backprop|backprop]], when the availability of labeled samples is below 10% (I assume of the available samples).\n\nPooling layers are [[zotero_notes/hebbian/hebbian-semi-supervised-in-sample-efficent-annotes-ECAMC5NF#^a1899a|crucial]] to its performance, interestingly enough.\n# chat breakdown\n\nThe HPCA (Hebbian Principal Component Analysis) learning rule is given by:\n\n$$\n\\Delta \\mathbf{w}_i = \\eta y_i \\left( \\mathbf{x} - \\sum_{j=1}^{i} y_j \\mathbf{w}_j \\right)\n$$\n\nIn this rule:\n\n- $\\Delta \\mathbf{w}_i$ is the change in the weight vector of the $i^{th}$ neuron.\n- $\\eta$ is the learning rate.\n- $y_i$ is the output of the $i^{th}$ neuron.\n- $\\mathbf{x}$ is the input vector.\n- $\\sum_{j=1}^{i} y_j \\mathbf{w}_j$ is the subtracted summation term, which represents the reconstruction of the input vector up to the $i^{th}$ component.\n\nIn traditional Hebbian learning, the rule is often expressed as \"cells that fire together wire together,\" implying that the synaptic weight is increased when both the presynaptic and postsynaptic neurons are active. A common extension to this rule includes a decay term to prevent unbounded growth of the weights.\n\nIn HPCA, the subtracted summation term $\\sum_{j=1}^{i} y_j \\mathbf{w}_j$ effectively replaces the decay term, but it does more than just control the growth of the weights. This term ensures that each weight vector learns to represent different principal components of the input data:\n\n1. **Orthogonality and Decorrelation**: The term $\\mathbf{x} - \\sum_{j=1}^{i} y_j \\mathbf{w}_j$ represents the residual error after removing the contributions of all the previous components (from 1 to $i$) from the input $\\mathbf{x}$. This ensures that each weight vector $\\mathbf{w}_i$ learns to be orthogonal to the previous weight vectors $\\mathbf{w}_1, \\mathbf{w}_2, ..., \\mathbf{w}_{i-1}$. Orthogonality is a key property of the principal components in PCA, where each component is uncorrelated with the others.\n\n2. **Variance Maximization**: By projecting the input $\\mathbf{x}$ onto the residual error, the algorithm ensures that each weight vector aligns with the direction of maximum variance in the residual. This is akin to how each principal component in PCA captures the direction of maximum variance in the data that is not already captured by the previous components.\n\n3. **Incremental Learning**: Unlike batch PCA, which computes all principal components at once using the entire dataset, HPCA learns the components incrementally. This allows it to be applied in online or streaming data scenarios, adapting to new data as it becomes available.\n\nThus, the subtracted summation in the HPCA learning rule effectively guides each neuron in the network to learn different principal components of the input data, following the spirit of PCA in terms of finding orthogonal directions of maximum variance, but in a neural network context and with an incremental, online learning capability.","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["hebbian","phd"]},"/Hebbian-Learning":{"title":"Hebbian Learning","content":" #hebbian \n\nIt's an alternative to backprop that uses only local update rules. It seems far more similar to spiking neural networks in that it tries to follow a more \"fires together, wires together\" paradigm. What's really appealing to me about this weight update mechanism is that it feels more online learning-friendly. I've long wondered why we try to represent short-term memory as activations only, when we know that they ought to have a pathway towards being encoded in the weights. \n\nI think that weight update rules that *never* turn off, even during inference time and in production environments, may be the key to bridging short-term and long-term memory in neural nets. Think, a transformer with no memory limits despite its context length.\n\nI'm aware that this concept has a big history. Biologically-plausible neural net design is a common obsession, but it has mixed results, with current backprop and fully-connected linear networks kinda spitting in its face. So, it seems like a common folly to keep obsessing about trying to get neural nets to behave like my own brain, yet, I can't help but thinking about the local weight update rules in Hebbian learning potentially producing some interesting results in terms of some sort of knowledge retention.\n\n# reading list\n[[Omnivore/2023-08-04/Comparing the performance of Hebbian against backpropagation learning using conv nns|Comparing the performance of Hebbian against backpropagation learning using conv nns]]\n\n[[Omnivore/2023-08-04/Evaluating Hebbian Learning in a Semi-supervised Setting - SpringerLink|Evaluating Hebbian Learning in a Semi-supervised Setting - SpringerLink]]","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["hebbian"]},"/HybridML-Paper":{"title":"HybridML Paper","content":" #work/patientsim #paper_notes\n\n[[2024-02-05]]\n\nNot much, just a way to combine mechanistic models with ML, using easy json and TensorFlow. Bizarrely, they claim to be an open platform whilst paywalling the paper.\\\n\n[GitHub - JRC-COMBINE/HybridML: Hybrid Modeling Framework](https://github.com/JRC-COMBINE/HybridML)","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["paper_notes"]},"/I-Built-a-Hebbian-RNN-next-steps":{"title":"I Built a Hebbian RNN next steps","content":" #hebbian  \n2024-02-09 08:06\n\n\nFour things to work on next:\n- Try out some new rules found in the survey, [[List of Hebbian Weight Update Rules|listed here]]\n- Use a more optimized dataset, perhaps simpler. Or make mine more efficient.\n- Run it on university compute.\n- Try some completely different rules from scratch, based on some other papers' code.\n\n## new rules\nDoes [[Ojas Rule]] work? I might even be able to generalize, probably not this next step, though. I may want to combine the general equation with a metalearning approach, like in [[zotero_notes/hebbian/meta-learning-through-hebbian-plasticity-KG2BPTBB|meta-learning-through-hebbian-plasticity]].\n![[List of Hebbian Weight Update Rules#General Local Synaptic Update Equation]]\n\nThat's not a simple approach. It would be simpler for me to just swap out rules like:\n- [[Ojas Rule]]\n- [[List of Hebbian Weight Update Rules#Competitive Learning Rule|competitive]]\n- [[List of Hebbian Weight Update Rules#Covariance Rule|covariance rule]]\n- maybe [[HPCA]]\nI can straight-shot just test them in the [[Current Build of the Memory Encoding Hebbian RNN|current build]], maybe just excluding my existing normalization of the weights in hopes that these rules eliminate the need for that. \n\nI also had an interesting idea where I replace my next-character prediction reward signal with one that was layer-wise. It's based on HPCA, but time-shifted: [[HPCA#crazy idea]]\n# better dataset\nI'm currently using project gutenberg, with 100-character strings chosen randomly. It's one-hot encoded, fed in one character at a time. I'm not sure whether it's a bottleneck in the learning process, for a couple of reasons:\n- it may be pulling from disk each time\n- it could potentially fit on gpu, which would be faster\n- it might be too complex for my current fast-iteration needs\nI should look into:\n- optimized, common datasets/dataloaders that are probably already built into pytorch\n- data that more explicitly suits the needs: it should incentivize taking long-term dependencies into account. \nI'm not sure what sort of dataset I'd be looking for in the latter case. I just want to see the thing train faster, especially on the backprop-rnn baseline. Perhaps a baby name dataset? But that doesn't have long-term dependencies. Low vocab, like simple english wikipedia may be it. Maybe some common computer vision dataset would do the trick. I don't really want to do reinforcement learning environments.\n\npytorch has built-in text datasets: [torchtext.datasets — Torchtext 0.17.0 documentation](https://pytorch.org/text/stable/datasets.html)\nNotably, the unsupervized ones look interesting:\n- [CC100](https://pytorch.org/text/stable/datasets.html#cc100) common crawl\n- [EnWik9](https://pytorch.org/text/stable/datasets.html#enwik9) wikipedia\nThe computer vision ones are interesting, too:\n[Datasets — Torchvision 0.17 documentation](https://pytorch.org/vision/stable/datasets.html)\nEspecially [Omniglot](https://pytorch.org/vision/stable/generated/torchvision.datasets.Omniglot.html#torchvision.datasets.Omniglot)\n![[Pasted image 20240209084726.png|500]]\nThis would have long-range dependencies, right? Like if I made it just put out one pixel at a time, left to right and down?\n# university compute\nI have to find my BYU id, contact Dr. Fulda, contact the office of research computing, use a vpn, it's a whole hassle. I'll want my code to be pretty organized and portable and optimized, so I guess I'm biased toward doing all the other things first. However, it's possible that my wishes for better understanding of the learning dynamics are just a 2-day supercomputer run away. I could literally be wasting time right now because my existing code just works.\n\n## wipe and restart\nOther papers already have hebbian learning running, I may just need to slap on my reward signal and rnn loop and get it going. It could be good to see how a new structure might prove superior. ","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["hebbian","General","Competitive","Covariance","crazy"]},"/Informer-paper":{"title":"Informer paper","content":" #work/patientsim #paper_notes\n[[2024-02-05]]\n\n[[2012.07436] Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://ar5iv.labs.arxiv.org/html/2012.07436) published in 2021\n\nIt's just a [[transformer|transformer]] with $O(L \\log{L})$ complexity in its attention mechanism, using some sparse version of self-attention via distilling it. It's proven to work well on time-series data.","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["paper_notes"]},"/Layer-Normalization":{"title":"Layer Normalization","content":" #hebbian \n\nThe technique stabilizes hidden state dynamics in RNNs. It may be an alternative solution to some previous issues I had with my [[Hebbian Learning|hebbian]] experiments, where the weight values, activations, and subsequently the loss numbers got increasingly erratic and large over time. \n\nHere's the quote:\n![[zotero_notes/Annotations-(1-31-2024,-9-54-17-AM)-2XQN472I#^7724ac]]\n\n","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["hebbian"]},"/List-of-Hebbian-Weight-Update-Rules":{"title":"List of Hebbian Weight Update Rules","content":" #hebbian \n\n### Simple Hebbian Learning Rule:\n$$ \\Delta w_i = \\eta y x_i $$\n\n### Weight Decay Term Added:\n$$ \\Delta w_i = \\eta y x_i - \\gamma(\\mathbf{w}, \\mathbf{x}) $$\n\n### Competitive Learning Rule:\n$$ \\Delta w_i = \\eta y x_i - \\eta y w_i = \\eta y (x_i - w_i) $$\n\n### Post-synaptic Gating:\n$$ \\Delta w_i = \\eta y (x_i - \\theta) $$\n\n### Pre-synaptic Gating Rule:\n$$ \\Delta w_i = \\eta x_i (y - \\theta) $$\n\n### Covariance Rule:\n$$ \\Delta w_i = \\eta (y - \\theta_y) (x_i - \\theta_x) $$\n\n### BCM Rule:\n$$ \\Delta w_i = \\eta x_i \\phi(y - \\theta) $$\n\n### Soft Thresholds on Weights:\n$$ \\Delta w_i = \\eta \\Delta w_i(\\text{base}) (w_i - \\theta_{\\text{LB}}) (\\theta_{\\text{UB}} - w_i) $$\n\n### Bi-stable Synapse Model (Part 1):\n$$ \\Delta w_i = H + R $$\n\n### Bi-stable Synapse Model (Refresh Term):\n$$ R = \\gamma w_i (1 - w_i) (w_i - \\theta) $$\n\n### Oja's Rule:\n$$ \\Delta w_i = \\eta y (x_i - y w_i) $$\n\n### General Local Synaptic Update Equation:\n$$ \\Delta w_i = a_0 + a_1 x_i + a_2 y + a_3 x_i y + a_4 x_i^2 + a_5 y^2 + \\ldots $$\n\n### Differential Hebbian Learning (DHL):\n$$ \\Delta w_i = \\eta \\frac{dy}{dt} x_i $$\n### HPCA\n$$\n\\Delta \\mathbf{w}_i = \\eta y_i \\left( \\mathbf{x} - \\sum_{j=1}^{i} y_j \\mathbf{w}_j \\right)\n$$\n### ABCD:\n$$ \\Delta w_{ij} = \\eta \\cdot (A_w x_i y + B_w x_i + C_w y + D_w) $$\n\n# whence\npulled from [[zotero_notes/hebbian/Survey-of-Hebbian-Learning-Lagani-2023-CF4RA22D#^12a1aa|Survey-of-Hebbian-Learning-Lagani-2023-CF4RA22D]]","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["hebbian"]},"/Merkelbach-Paper":{"title":"Merkelbach Paper","content":" #work/patientsim #paper_notes\n\n# Merkelbach version\nBased on the MIMIC dataset.\n## data prep\n- time series data collected, normalized\n- filled in missing data, imputed via linear interpolation, or in the case of no intag:#work/patientsimstance of this data at all, the median\n- static data tacked onto every single instance of the time series data\n## GRU\n(it's bidirectional)\nprocesses the whole of the data in both directions. Outputs are averaged over all the time steps, presumable leaving a single vector the length of the number of features.\n\n## autoencoder\nThe single vector is put through a dense layer, making the bottleneck.\nThat bottleneck is \"repeated through time\", which I just think means it's copy/pasted as many times as there are timesteps, with the related timestep features added on. \n\n### decoder gru\nTakes in the expanded bottleneck, turns it into another set (size of n timesteps) of vectors. A dense layer turns that into a copy of the original data. \n\n## pic\n\n![[Pasted image 20240125124146.png]]\n## my issues\n- data is imputed - I'd personally rather it be robust to missingness\n- static data repeated too much. We shouldn't have to repeat ourselves.\n- the gru is just summed. what a weird way to combine all those values. Really messy. There's gotta be a better way.\n- really, why is a gru even used?? Simply because it's the only thing that could fit a time series?\n\n[[Triplet Sparsity Reduction]] is my proposed solution to those issues.","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["paper_notes"]},"/Ojas-Rule":{"title":"Ojas Rule","content":"#phd #hebbian  \n[[2024-02-07]]\n\nAn addition to the basic [[List of Hebbian Weight Update Rules#Simple Hebbian Learning Rule|hebbian]] learning rule that adds in the y to the subtracted term. \n![[List of Hebbian Weight Update Rules#Oja's Rule]]\n\nWhen learning rates are low enough, it sorta approximates a normalization of the weights, so that the weights don't [[zotero_notes/hebbian/Survey-of-Hebbian-Learning-Lagani-2023-CF4RA22D#^cecc45|grow out of control]]. \n![[zotero_notes/hebbian/Survey-of-Hebbian-Learning-Lagani-2023-CF4RA22D#^032510|Survey-of-Hebbian-Learning-Lagani-2023-CF4RA22D]]\n\nIf you want to align towards multiple principal components, [[HPCA|you can do that]], but you'll need to include more than just one neuron.","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["phd","hebbian","Simple"]},"/Omnivore/2023-08-04/Comparing-the-performance-of-Hebbian-against-backpropagation-learning-using-conv-nns":{"title":"Comparing the performance of Hebbian against backpropagation learning using conv nns","content":"\n\n# Comparing the performance of Hebbian against backpropagation learning using conv nns\n#Omnivore\n #hebbian \n  \n[Read on Omnivore](https://omnivore.app/me/comparing-the-performance-of-hebbian-against-backpropagation-lea-189c2643103)\n\n[Read Original](https://link.springer.com/article/10.1007/s00521-021-06701-4)\n\n## Highlights\n\n\u003e The results on various datasets are compared with those obtained by unsupervised VAE, and the potentials and limitations of the methods are highlighted; we also deemed interesting to report the results of supervised backprop training in our discussion; [⤴️](https://omnivore.app/me/comparing-the-performance-of-hebbian-against-backpropagation-lea-189c2643103#02caac6f-8b80-42ad-b10e-c702f4ee120e)  ^02caac6f\n\n\u003e Pros of Hebbian learning:\n\u003e \n\u003e * Effective for training low-level feature extractors;\n\u003e * Produces better features than VAE for the classification task;\n\u003e * Effective for re-training higher network layers in fewer epochs than other approaches;\n\u003e * Some hybrid combinations of Hebbian and backprop help improving performance in some cases, as can be observed in Appendix [1](https://link.springer.com/article/10.1007/s00521-021-06701-4#Sec33);\n\u003e \n\u003e Cons of Hebbian learning:\n\u003e \n\u003e * Not effective for training intermediate layers;\n\u003e * Even though HPCA provides a reduction in the gap between unsupervised and supervised methods, the latter are still preferable for end-to-end network training;\n\u003e * Finding the best combination of Hebbian and backprop layers is not immediate and requires exploring various network configurations. [⤴️](https://omnivore.app/me/comparing-the-performance-of-hebbian-against-backpropagation-lea-189c2643103#2fb11229-5007-48de-a177-4055ad5da22b)  ^2fb11229\n\n\u003e our results suggest that the Hebbian approach is suitable for training early feature extraction layers or to re-train the final layers of a pre-trained deep neural network, requiring fewer training epochs than other methods. [⤴️](https://omnivore.app/me/comparing-the-performance-of-hebbian-against-backpropagation-lea-189c2643103#4b1ee85a-95b2-41aa-8570-f90b020d6f9d)  ^4b1ee85a\n\n\u003e Hebbian approaches outperform VAE training [⤴️](https://omnivore.app/me/comparing-the-performance-of-hebbian-against-backpropagation-lea-189c2643103#e0d3b015-3325-44a2-b4ee-2f5b2e82f965)  ^e0d3b015\n\n\u003e github.com/GabrieleLagani/HebbianPCA/tree/hebbpca [⤴️](https://omnivore.app/me/comparing-the-performance-of-hebbian-against-backpropagation-lea-189c2643103#396bc53b-e73d-4967-b794-22c690ef982a)  ^396bc53b\n\nhttps://github.com/GabrieleLagani/HebbianPCA/tree/hebbpca\n\n","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["Omnivore","hebbian"]},"/Omnivore/2023-08-04/Comparing-the-performance-of-Hebbian-against-backpropagation-learning-using-convolutional-neural-...":{"title":"Comparing the performance of Hebbian against backpropagation learning using convolutional neural ...","content":"\n\n# Comparing the performance of Hebbian against backpropagation learning using convolutional neural networks | SpringerLink\n#Omnivore\n #hebbian \n  \n[Read on Omnivore](https://omnivore.app/me/comparing-the-performance-of-hebbian-against-backpropagation-lea-189c2643103)\n[Read Original](https://link.springer.com/article/10.1007/s00521-021-06701-4)\n\n## Highlights\n\n\u003e The results on various datasets are compared with those obtained by unsupervised VAE, and the potentials and limitations of the methods are highlighted; we also deemed interesting to report the results of supervised backprop training in our discussion; [⤴️](https://omnivore.app/me/comparing-the-performance-of-hebbian-against-backpropagation-lea-189c2643103#02caac6f-8b80-42ad-b10e-c702f4ee120e)  ^02caac6f\n\n","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["Omnivore","hebbian"]},"/Omnivore/2023-08-04/Evaluating-Hebbian-Learning-in-a-Semi-supervised-Setting-SpringerLink":{"title":"Evaluating Hebbian Learning in a Semi-supervised Setting - SpringerLink","content":"\n\n# Evaluating Hebbian Learning in a Semi-supervised Setting | SpringerLink\n#Omnivore\n #hebbian \n  \n[Read on Omnivore](https://omnivore.app/me/evaluating-hebbian-learning-in-a-semi-supervised-setting-springe-189c25f5df0)\n\n[Read Original](https://link.springer.com/chapter/10.1007/978-3-030-95470-3_28)\n\n## Highlights\n\n\u003e In this work, we propose a semi-supervised learning approach, in which the unsupervised pre-training step is performed by means of the Hebbian learning paradigm. [⤴️](https://omnivore.app/me/evaluating-hebbian-learning-in-a-semi-supervised-setting-springe-189c25f5df0#ddb528d6-81a8-4b6b-baa4-d32d23b16089)  ^ddb528d6\n\n","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["Omnivore","hebbian"]},"/PatchMixer":{"title":"PatchMixer","content":" #work/patientsim #paper_notes\n[[2024-02-05]]\n\nA convolutional neural network!! Haven't seen one in a minute, and certainly didn't expect it in time series processing.\n\n![[zotero_notes/patient_sim/PatchMixer-G578PTIA#^08f144|PatchMixer-G578PTIA]]\n\nSeems like the main innovation is in its [[zotero_notes/patient_sim/PatchMixer-G578PTIA#^68a782|patch-mixing]], used as an alternative to self-attention. Patch-mixing relies on convolutions. First, there's convolutions over each patch, then convolutions over those representations of the patches.\n\n### 1. **Convolutions Over Each Patch**:\n\n- **Depthwise Convolution**: This step involves applying convolutions to each patch of the input data separately.\n- **Local Feature Extraction**: The purpose here is to capture the local features within each patch. Each patch is processed independently, allowing the model to learn detailed, location-specific patterns or characteristics within that segment of the data.\n\n### 2. **Convolutions Over Representations of the Patches**:\n\n- **Pointwise Convolution**: After processing each patch independently, the next step involves applying a pointwise convolution across these patch representations.\n- **Global Feature Integration**: This stage is crucial for combining the information from different patches. It helps in understanding the relationships between patches, essentially capturing the global or long-range dependencies in the data.\n\nThis is not for heterogeneous data. seems like it's mostly for continuous time series. In fact, it's channel-independent, completely separating different features and not using them for predicting each other. Not ideal for patient similarity representation. ![[zotero_notes/patient_sim/PatchMixer-G578PTIA#^f51cf8|PatchMixer-G578PTIA]]\n\n\n\n[[2310.00655] PatchMixer: A Patch-Mixing Architecture for Long-Term Time Series Forecasting](https://ar5iv.labs.arxiv.org/html/2310.00655v1)\n\n","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["paper_notes"]},"/Patient-Similarity-Readings-Log":{"title":"Patient Similarity Readings Log","content":" #work/patientsim \n\n\n```dataview\nlist file.title + dateformat(file.ctime, \" yy-MM-dd\") + \" - \" + file.tags\nfrom #work/patientsim and #paper_notes\nsort file.ctime desc\n```\n","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["paper_notes"]},"/Perceiver-Paper":{"title":"Perceiver Paper","content":" #work/patientsim \n[[2024-02-05]]\n\nOn the surface, it seems like it's pretty much just: [[transformer]] for pictures and videos and images and whatnot. \n- It's flexible. -\n- It scales to many more inputs than the traditional NLP transformer.\n- Cross-attention distills inputs into smaller latent space. Sounds like [[stable diffusion]].\n- It claims to be a generalization of transformers. bold claim\n- is it useful for text? Like, if it's so much better than transformers, how does it do in their home turf? [[PerceiverIO Paper|It seems to match it.]]\n\n[[2103.03206] Perceiver: General Perception with Iterative Attention](https://ar5iv.labs.arxiv.org/html/2103.03206)","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":[]},"/Perceiver-vs-Informer":{"title":"Perceiver vs Informer","content":" #work/patientsim \n# they're different\nThey were developed independently for different purposes:\n\n- The [[Perceiver Paper|Perceiver]], developed by DeepMind, focuses on generalizing the transformer architecture to handle various types of input data without task-specific modifications.\n- The [[Informer paper|Informer]], developed by researchers primarily from Tsinghua University, is tailored for efficient long sequence time-series forecasting, addressing specific challenges in handling long sequences with transformers.\n\n# they are used for different things\n1. **Perceivers**: Developed by DeepMind in 2021, Perceivers were created to generalize the transformer model to handle various input types (like images, audio, text) in a uniform way. This approach allows processing of different modalities and input sizes without task-specific architectures, using a fixed-size latent space.\n2. **Informers**: Developed by researchers at Tsinghua University and other institutions in 2020, Informers are designed to improve the efficiency of transformers for long sequence time-series forecasting. They address the challenge of computational complexity in handling long sequences with innovations like ProbSparse self-attention, enabling the model to focus on the most relevant parts of the input.","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":[]},"/PerceiverIO-Paper":{"title":"PerceiverIO Paper","content":" #work/patientsim \n[[2024-02-05]]\n\n# on NLP\nThey seem to pretty much match transformers, flop-for-flop.\n\n\u003e We first compare Perceiver IO to standard Transformers for language. Although Transformers were originally developed for language, their quadratic complexity makes them difficult to use on language inputs without tokenization, which typically shortens the length of input sequences by a factor of ∼4. But unlike Transformer-based models such as BERT (Devlin et al., [2019](https://ar5iv.labs.arxiv.org/html/2107.14795#bib.bib21)) or XLNet (Yang et al., [2019](https://ar5iv.labs.arxiv.org/html/2107.14795#bib.bib97)), Perceiver IO scales linearly with input length. Our experiments focus on showing that Perceiver IO performs as well as or better than Transformers for masked language modeling (MLM) while removing tokenization (which is hard to maintain, introduces engineering overhead, and adds needless complexity to language models (Bostrom \u0026 Durrett, [2020](https://ar5iv.labs.arxiv.org/html/2107.14795#bib.bib6); Clark et al., [2022](https://ar5iv.labs.arxiv.org/html/2107.14795#bib.bib14))).\n\u003e We compare results for a given FLOPs budget rather than a given parameter budget as the former grows quadratically with sequence length but the latter is independent (except for positional encodings). From a practioner’s perspective, FLOPs matter more than parameters since FLOPs directly relate to training time. We evaluate the quality of the learned representation on the GLUE benchmark (Wang et al., [2019](https://ar5iv.labs.arxiv.org/html/2107.14795#bib.bib92)) and report our results in Tab. [1](https://ar5iv.labs.arxiv.org/html/2107.14795#S4.T1 \"Table 1 ‣ 4.1 Language ‣ 4 Experiments ‣ Perceiver IO: A General Architecture for Structured Inputs \u0026 Outputs\"). We find that at a given FLOPs budget, Perceiver IO trained without tokenization matches the performance of a strong Transformer-based model trained with SentencePiece tokenization (Sennrich et al., [2016](https://ar5iv.labs.arxiv.org/html/2107.14795#bib.bib69); Kudo \u0026 Richardson, [2018](https://ar5iv.labs.arxiv.org/html/2107.14795#bib.bib43)).\n","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":[]},"/Synaptic-Balancing":{"title":"Synaptic Balancing","content":" #hebbian #phd  \n2024-02-12 14:19\n\nThis was a very confusing and math-heavy paper.\n\n\n# chat transcript\nCertainly! Let's outline a simple algorithm for updating synaptic weights in a neural network using a synaptic balancing rule, along with the computation and tracking of average gain. We'll use a ReLU activation function for this example:\n\n### Variables and Setup\n- Let \\( w_{ij} \\) be the weight of the synapse from neuron \\( j \\) to neuron \\( i \\).\n- \\( \\sigma_j \\) will represent the average gain (average activation) of neuron \\( j \\).\n- \\( a_j \\) is the activation of neuron \\( j \\).\n- Activation function for neuron \\( j \\), \\( f(x) \\), is ReLU: \\( f(x) = \\max(0, x) \\).\n- Learning rate: \\( \\eta \\).\n\n### Algorithm Steps\n\n1. **Initialization**:\n   - Initialize all synaptic weights \\( w_{ij} \\) randomly.\n   - Initialize all average gains \\( \\sigma_j \\) to 0.\n\n2. **Forward Pass for Each Training Example**:\n   - Compute the activation \\( a_j \\) for each neuron using ReLU.\n   - Store these activations for calculating the average gain.\n\n3. **Calculate Average Gain**:\n   - For each neuron \\( j \\), update its average gain:\n     $$ \\sigma_j = \\text{MovingAverage}(\\sigma_j, a_j) $$\n   - Here, \\(\\text{MovingAverage}\\) is a function that computes the moving average, incorporating the latest activation \\( a_j \\) into the existing average \\( \\sigma_j \\).\n\n4. **Synaptic Balancing Rule for Weight Update**:\n   - For each synapse \\( w_{ij} \\), update the weight based on the average gains of the pre- and post-synaptic neurons:\n     $$ \\Delta w_{ij} = \\eta \\cdot (\\sigma_j^2 - \\sigma_i^2) \\cdot w_{ij} $$\n   - Update the weight: \\( w_{ij} \\leftarrow w_{ij} + \\Delta w_{ij} \\).\n\n5. **Repeat for Each Training Iteration**:\n   - Repeat steps 2 to 4 for each training iteration, using the respective inputs and calculating the activations for each neuron.\n\n### Additional Notes\n- **Moving Average Calculation**: The moving average can be calculated using an exponential moving average or a simple windowed average. For instance, an exponential moving average with a smoothing factor \\( \\alpha \\) would be:\n  $$ \\sigma_j = \\alpha \\cdot a_j + (1 - \\alpha) \\cdot \\sigma_j $$\n- **Training Data**: The algorithm runs through the training data, performing the forward pass and weight updates for each training example or batch of examples.\n- **Learning Rate**: The learning rate \\( \\eta \\) controls the speed of learning. It should be chosen carefully to balance the speed of convergence and the stability of learning.\n\nThis algorithm provides a framework for implementing synaptic balancing in a simple neural network, where the synaptic weight updates are influenced by the average activation levels (interpreted as gains) of the neurons. The balance between the pre- and post-synaptic neurons' average activations guides the adjustments of synaptic strengths, aiming to maintain a balanced and functional network.\n\n\n\n\nIn the context of the synaptic balancing rule and considering your focus on ReLU activation functions, using the derivative of the activation function to represent gain is indeed less informative due to its binary nature (0 or 1). However, if we were to include the derivative in the context of a different activation function, or if you prefer to include it even with ReLU, here's how it could be integrated into the algorithm:\n\n### Algorithm with Derivative of Activation Function for Gain\n\n1. **Initialization**:\n   - Initialize synaptic weights \\( w_{ij} \\) and average gains \\( \\sigma_j \\) as before.\n\n2. **Forward Pass and Gain Calculation for Each Training Example**:\n   - Compute the activation \\( a_j \\) for each neuron using the activation function.\n   - Calculate the derivative of the activation function at \\( a_j \\). For ReLU, this is 0 for \\( a_j \u003c 0 \\) and 1 for \\( a_j \\geq 0 \\). For other functions, calculate the actual derivative.\n   - Update the average gain \\( \\sigma_j \\) using this derivative value.\n\n3. **Update Average Gain**:\n   - Update the average gain for each neuron \\( j \\):\n     $$ \\sigma_j = \\text{MovingAverage}(\\sigma_j, f'(a_j)) $$\n\n4. **Synaptic Balancing Rule for Weight Update**:\n   - Update the synaptic weights as before:\n     $$ \\Delta w_{ij} = \\eta \\cdot (\\sigma_j^2 - \\sigma_i^2) \\cdot w_{ij} $$\n   - Apply the weight update: \\( w_{ij} \\leftarrow w_{ij} + \\Delta w_{ij} \\).\n\n5. **Repeat for Each Training Iteration**.\n\n### Considerations\n\n- **Derivative in ReLU**: For ReLU, this approach would mean that the average gain \\( \\sigma_j \\) effectively tracks how often the neuron is active (since the derivative is 1 when the neuron is active and 0 otherwise).\n- **Other Activation Functions**: If you use activation functions other than ReLU, like sigmoid or tanh, the derivative provides a more nuanced measure of gain, reflecting how sensitive the neuron's output is to changes in its input.\n\nUsing the derivative of the activation function to represent gain, especially with activation functions that have more variable derivatives, provides a measure of how a neuron's output changes in response to its inputs, which can be crucial for synaptic adjustments in learning algorithms.","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["hebbian","phd"]},"/Transformer-based-Solutions":{"title":"Transformer-based Solutions","content":"#work/patientsim  \n\n- [[COPER Paper]] just used perceivers, and physics models\n- we could use [[Informer paper|informers]], which effectively lengthen the context length, and are especially good at time-series.\n- [[triplet transformer for patient data|triplet transformer]] uses a transformer with some innovative [[Triplet Sparsity Reduction|sparsity reduction]].\n","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":[]},"/Triplet-Sparsity-Reduction":{"title":"Triplet Sparsity Reduction","content":"#work/patientsim  \n\nThis note was originally created in contrast to the gru-based autoencoder in the [[Merkelbach Paper]]. That's right, I had the idea before I found it in the paper. I got a big head about it.\n# My Version\n\n- RNN (of some sort), each data point gets a timestep of its own.\n- Each data point/feature gets turned into an embedding. \n\t- basically im just saying that instead of having a big sparse representation, i'm densifying it early. like character one-hots to word embeddings. \n\t- it also gets its own positional encoding, which may be identical to the previous iteration of the rnn bc it's collected at the same time. \n- maybe the static data gets tacked onto the output after recurring stuff is summed, or otherwise collected into a standard vector of fixed size.\n- we're gonna have issues with long-term dependencies. That's just a fact. \n\n# existing similar approaches\nAnother paper already does, this for use in a transformer-based approach! That's where I get the [[triplet transformer for patient data|triplet]] name from.\n\nIt's super similar to the way that tokens themselves are treated in a [[transformer|transformer]]. The time, feature type embedding, and the value of the instance of the feature, are akin to [[keys, queries, values]].\n### 3.2. Architecture of STraTS\n\nThe architecture of STraTS is illustrated in Figure [3](https://ar5iv.labs.arxiv.org/html/2107.14293#S3.F3 \"Figure 3 ‣ 3.2.1. Initial Triplet Embedding ‣ 3.2. Architecture of STraTS ‣ 3. Proposed Approach ‣ Self-Supervised Transformer for Sparse and Irregularly Sampled Multivariate Clinical Time-Series\"). Unlike most of the existing approaches which take a time-series matrix as input, STraTS defines its input as a set of observation triplets. Each observation triplet in the input is embedded using the Initial Triplet Embedding module. The initial triplet embeddings are then passed through a Contextual Triplet Embedding module which utilizes the Transfomer architecture to encode the context for each triplet. The Fusion Self-attention module then combines these contextual embeddings via self-attention mechanism to generate an embedding for the input time-series which is concatenated with demographics embedding and passed through a feed-forward network to make the final prediction. The notations used in the paper are summarized in Table [1](https://ar5iv.labs.arxiv.org/html/2107.14293#S3.T1 \"Table 1 ‣ 3.1. Problem Definition ‣ 3. Proposed Approach ‣ Self-Supervised Transformer for Sparse and Irregularly Sampled Multivariate Clinical Time-Series\").\n\n#### 3.2.1. Initial Triplet Embedding\n\nGiven an input time-series 𝐓={(ti,fi,vi)}i=1n, the initial embedding for the ith triplet 𝐞𝐢∈ℝd is computed by summing the following component embeddings: (i) Feature embedding 𝐞𝐢𝐟∈ℝd, (ii) Value embedding 𝐞𝐢𝐯∈ℝd, and (iii) Time embedding 𝐞𝐢𝐭∈ℝd. In other words, 𝐞𝐢=𝐞𝐢𝐟+𝐞𝐢𝐯+𝐞𝐢𝐭∈ℝd. Feature embeddings 𝐞𝐟(⋅) are obtained from a simple lookup table similar to word embeddings. Since feature values and times are continuous unlike feature names which are categorical objects, we cannot use a lookup table to embed these continuous values unless they are categorized. Some researchers (Vaswani et al., [2017](https://ar5iv.labs.arxiv.org/html/2107.14293#bib.bib31); Yin et al., [2020](https://ar5iv.labs.arxiv.org/html/2107.14293#bib.bib33)) have used sinusoidal encodings to embed continuous values. We propose a novel continuous value embedding (CVE) technique using a one-to-many Feed-Forward Network (FFN) with learnable parameters i.e., 𝐞𝐢𝐯=FFNv(vi), and 𝐞𝐢𝐭=FFNt(ti).\n\nBoth FFNs have one input neuron and d output neurons and a single hidden layer with ⌊d⌋ neurons and tanh(⋅) activation. They are of the form FFN(x)=Utanh(Wx+b) where the dimensions of weights {W,b,U} can be inferred from the size of hidden and output layers of the FFN. Unlike sinusoidal encodings with fixed frequencies, this technique offers more flexibility by allowing end-to-end learning of continuous value and time embeddings without the need to categorize them.\n\nFigure 3. The overall architecture of the proposed STraTS model. The Input Triplet Embedding module embeds each observation triplet, the Contextual Triplet Embedding module encodes contextual information for the triplets, the Fusion Self-Attention module computes time-series embedding which is concatenated with demographics embedding and passed through a dense layer to generate predictions for target and self-supervision (forecasting) tasks.\n\n![Refer to caption](https://ar5iv.labs.arxiv.org/html/2107.14293/assets/x3.png)\n\nFully described in text.\n\nFigure 3. The overall architecture of the proposed STraTS model. The Input Triplet Embedding module embeds each observation triplet, the Contextual Triplet Embedding module encodes contextual information for the triplets, the Fusion Self-Attention module computes time-series embedding which is concatenated with demographics embedding and passed through a dense layer to generate predictions for target and self-supervision (forecasting) tasks.\n\n#### 3.2.2. Contextual Triplet Embedding\n\nThe initial triplet embeddings {𝐞1,…,𝐞n} are then passed through a Transformer architecture (Vaswani et al., [2017](https://ar5iv.labs.arxiv.org/html/2107.14293#bib.bib31)) with M blocks, each containing a Multi-Head Attention (MHA) layer with h attention heads and a FFN with one hidden layer. Each block takes n input embeddings 𝐄∈ℝn×d and outputs the corresponding n output embeddings 𝐂∈ℝn×d that capture the contextual information. MHA layers use multiple attention heads to attend to information contained in different embedding projections in parallel. The computations of the MHA layer are given by\n### 3.2. Architecture of STraTS\n\nThe architecture of STraTS is illustrated in Figure [3](https://ar5iv.labs.arxiv.org/html/2107.14293#S3.F3 \"Figure 3 ‣ 3.2.1. Initial Triplet Embedding ‣ 3.2. Architecture of STraTS ‣ 3. Proposed Approach ‣ Self-Supervised Transformer for Sparse and Irregularly Sampled Multivariate Clinical Time-Series\"). Unlike most of the existing approaches which take a time-series matrix as input, STraTS defines its input as a set of observation triplets. Each observation triplet in the input is embedded using the Initial Triplet Embedding module. The initial triplet embeddings are then passed through a Contextual Triplet Embedding module which utilizes the Transfomer architecture to encode the context for each triplet. The Fusion Self-attention module then combines these contextual embeddings via self-attention mechanism to generate an embedding for the input time-series which is concatenated with demographics embedding and passed through a feed-forward network to make the final prediction. The notations used in the paper are summarized in Table [1](https://ar5iv.labs.arxiv.org/html/2107.14293#S3.T1 \"Table 1 ‣ 3.1. Problem Definition ‣ 3. Proposed Approach ‣ Self-Supervised Transformer for Sparse and Irregularly Sampled Multivariate Clinical Time-Series\").\n\n#### 3.2.1. Initial Triplet Embedding\n\nGiven an input time-series 𝐓={(ti,fi,vi)}i=1n, the initial embedding for the ith triplet 𝐞𝐢∈ℝd is computed by summing the following component embeddings: (i) Feature embedding 𝐞𝐢𝐟∈ℝd, (ii) Value embedding 𝐞𝐢𝐯∈ℝd, and (iii) Time embedding 𝐞𝐢𝐭∈ℝd. In other words, 𝐞𝐢=𝐞𝐢𝐟+𝐞𝐢𝐯+𝐞𝐢𝐭∈ℝd. Feature embeddings 𝐞𝐟(⋅) are obtained from a simple lookup table similar to word embeddings. Since feature values and times are continuous unlike feature names which are categorical objects, we cannot use a lookup table to embed these continuous values unless they are categorized. Some researchers (Vaswani et al., [2017](https://ar5iv.labs.arxiv.org/html/2107.14293#bib.bib31); Yin et al., [2020](https://ar5iv.labs.arxiv.org/html/2107.14293#bib.bib33)) have used sinusoidal encodings to embed continuous values. We propose a novel continuous value embedding (CVE) technique using a one-to-many Feed-Forward Network (FFN) with learnable parameters i.e., 𝐞𝐢𝐯=FFNv(vi), and 𝐞𝐢𝐭=FFNt(ti).\n\nBoth FFNs have one input neuron and d output neurons and a single hidden layer with ⌊d⌋ neurons and tanh(⋅) activation. They are of the form FFN(x)=Utanh(Wx+b) where the dimensions of weights {W,b,U} can be inferred from the size of hidden and output layers of the FFN. Unlike sinusoidal encodings with fixed frequencies, this technique offers more flexibility by allowing end-to-end learning of continuous value and time embeddings without the need to categorize them.\n\nFigure 3. The overall architecture of the proposed STraTS model. The Input Triplet Embedding module embeds each observation triplet, the Contextual Triplet Embedding module encodes contextual information for the triplets, the Fusion Self-Attention module computes time-series embedding which is concatenated with demographics embedding and passed through a dense layer to generate predictions for target and self-supervision (forecasting) tasks.\n\n![Refer to caption](https://ar5iv.labs.arxiv.org/html/2107.14293/assets/x3.png)\n\nFully described in text.\n\nFigure 3. The overall architecture of the proposed STraTS model. The Input Triplet Embedding module embeds each observation triplet, the Contextual Triplet Embedding module encodes contextual information for the triplets, the Fusion Self-Attention module computes time-series embedding which is concatenated with demographics embedding and passed through a dense layer to generate predictions for target and self-supervision (forecasting) tasks.\n\n#### 3.2.2. Contextual Triplet Embedding\n\nThe initial triplet embeddings {𝐞1,…,𝐞n} are then passed through a Transformer architecture (Vaswani et al., [2017](https://ar5iv.labs.arxiv.org/html/2107.14293#bib.bib31)) with M blocks, each containing a Multi-Head Attention (MHA) layer with h attention heads and a FFN with one hidden layer. Each block takes n input embeddings 𝐄∈ℝn×d and outputs the corresponding n output embeddings 𝐂∈ℝn×d that capture the contextual information. MHA layers use multiple attention heads to attend to information contained in different embedding projections in parallel. The computations of the MHA layer are given by\n\n|   |   |   |   |\n|---|---|---|---|\n|(1)||𝐇j=softmax((𝐄𝐖jq)(𝐄𝐖jk)Td/h)(𝐄𝐖jv)j=1,…,h||\n|(2)||MHA(𝐄)=(𝐇1∘…∘𝐇h)𝐖c||\n\nEach head projects the input embeddings into query, key, and value subspaces using matrices {𝐖jq,𝐖jk,𝐖jv}⊂ℝd×dh. The queries and keys are then used to compute the attention weights which are used to compute weighted averages of value (different from value in observation triplet) vectors. Finally, the outputs of all heads are concatenated and projected to original dimension with 𝐖c∈ℝhdh×d. The FFN layer takes the form\n\n|   |   |   |   |   |\n|---|---|---|---|---|\n|(3)||𝐅(𝐗)|=ReLU(𝐗𝐖1f+𝐛1f)𝐖2f+𝐛2f||\n\nwith weights 𝐖𝟏𝐟∈ℝd×2d,𝐛𝟏𝐟∈ℝ2d,𝐖𝟐𝐟∈ℝ2d×d,𝐛𝟐𝐟∈ℝd. Dropout, residual connections, and layer normalization are added for every MHA and FFN layer. Also, attention dropout randomly masks out some positions in the attention matrix before the softmax computation during training. The output of each block is fed as input to the succeeding one, and the output of the last block gives the contextual triplet embeddings {𝐜1,…,𝐜n}.\n\n#### 3.2.3. Fusion Self-attention\n\nAfter computing contextual embeddings using a Transformer, we fuse them using a self-attention layer to compute time-series embedding 𝐞T∈ℝd. This layer first computes attention weights {α1,…,αn} by passing each contextual embedding through a FFN and computing a softmax over all the FFN outputs.\n\n|   |   |   |   |   |\n|---|---|---|---|---|\n|(4)||ai|=𝐮aTtanh(𝐖a𝐜i+𝐛a)||\n|(5)||αi|=exp(ai)∑j=1nexp(aj)∀i=1,…,n||\n\n𝐖a∈ℝda×d,𝐛a∈ℝda,𝐮𝐚∈ℝda are the weights of this attention network which has da neurons in the hidden layer. The time-series embedding is then computed as\n\n|   |   |   |   |\n|---|---|---|---|\n|(6)||𝐞T=∑i=1nαi𝐜𝐢||\n\n#### 3.2.4. Demographics Embedding\n\nWe realize that demographics can be encoded as triplets with a default value for time. However, we found that the prediction models performed better in our experiments when demographics are processed separately by passing 𝐝 through a FFN as shown below. The demographics embedding is thus obtained as\n\n|   |   |   |   |\n|---|---|---|---|\n|(7)||𝐞d=tanh(𝐖2dtanh(𝐖1d𝐝+𝐛1d)+𝐛2d)∈ℝd||\n\nwhere the hidden layer has a dimension of 2d.\n\n#### 3.2.5. Prediction Head\n\nThe final prediction for target task is obtained by passing the concatenation of demographics and time-series embeddings through a dense layer with weights 𝐰oT∈ℝd, bo∈ℝ and sigmoid activation.\n\n|   |   |   |   |\n|---|---|---|---|\n|(8)||y~=sigmoid(𝐰oT[𝐞d∘𝐞T]+bo)||\n\nThe model is trained on the target task using cross-entropy loss.\n|   |   |   |   |\n|---|---|---|---|\n|(1)||𝐇j=softmax((𝐄𝐖jq)(𝐄𝐖jk)Td/h)(𝐄𝐖jv)j=1,…,h||\n|(2)||MHA(𝐄)=(𝐇1∘…∘𝐇h)𝐖c||\n\nEach head projects the input embeddings into query, key, and value subspaces using matrices {𝐖jq,𝐖jk,𝐖jv}⊂ℝd×dh. The queries and keys are then used to compute the attention weights which are used to compute weighted averages of value (different from value in observation triplet) vectors. Finally, the outputs of all heads are concatenated and projected to original dimension with 𝐖c∈ℝhdh×d. The FFN layer takes the form\n\n|   |   |   |   |   |\n|---|---|---|---|---|\n|(3)||𝐅(𝐗)|=ReLU(𝐗𝐖1f+𝐛1f)𝐖2f+𝐛2f||\n\nwith weights 𝐖𝟏𝐟∈ℝd×2d,𝐛𝟏𝐟∈ℝ2d,𝐖𝟐𝐟∈ℝ2d×d,𝐛𝟐𝐟∈ℝd. Dropout, residual connections, and layer normalization are added for every MHA and FFN layer. Also, attention dropout randomly masks out some positions in the attention matrix before the softmax computation during training. The output of each block is fed as input to the succeeding one, and the output of the last block gives the contextual triplet embeddings {𝐜1,…,𝐜n}.\n\n#### 3.2.3. Fusion Self-attention\n\nAfter computing contextual embeddings using a Transformer, we fuse them using a self-attention layer to compute time-series embedding 𝐞T∈ℝd. This layer first computes attention weights {α1,…,αn} by passing each contextual embedding through a FFN and computing a softmax over all the FFN outputs.\n\n|   |   |   |   |   |\n|---|---|---|---|---|\n|(4)||ai|=𝐮aTtanh(𝐖a𝐜i+𝐛a)||\n|(5)||αi|=exp(ai)∑j=1nexp(aj)∀i=1,…,n||\n\n𝐖a∈ℝda×d,𝐛a∈ℝda,𝐮𝐚∈ℝda are the weights of this attention network which has da neurons in the hidden layer. The time-series embedding is then computed as\n\n|   |   |   |   |\n|---|---|---|---|\n|(6)||𝐞T=∑i=1nαi𝐜𝐢||\n\n#### 3.2.4. Demographics Embedding\n\nWe realize that demographics can be encoded as triplets with a default value for time. However, we found that the prediction models performed better in our experiments when demographics are processed separately by passing 𝐝 through a FFN as shown below. The demographics embedding is thus obtained as\n\n|   |   |   |   |\n|---|---|---|---|\n|(7)||𝐞d=tanh(𝐖2dtanh(𝐖1d𝐝+𝐛1d)+𝐛2d)∈ℝd||\n\nwhere the hidden layer has a dimension of 2d.\n\n#### 3.2.5. Prediction Head\n\nThe final prediction for target task is obtained by passing the concatenation of demographics and time-series embeddings through a dense layer with weights 𝐰oT∈ℝd, bo∈ℝ and sigmoid activation.\n\n|   |   |   |   |\n|---|---|---|---|\n|(8)||y~=sigmoid(𝐰oT[𝐞d∘𝐞T]+bo)||\n\nThe model is trained on the target task using cross-entropy loss.\n","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":[]},"/Visualize-a-Codebase":{"title":"Visualize a Codebase","content":" #project #python\n# Summary:\nIt's hard to understand codebases. I'd like an automatic tool for it, that can show it to me in a visual structure. \n\n# Tools\n- PyCG generates call graphs for Python code using static analysis. It efficiently supports Higher order functions Twisted class inheritance schemes Automatic discovery of imported modules for further analysis Nested definitions \n- Graphviz, especially dot files.\n- a javascript package can animate between different generated dot files: [GitHub - magjac/d3-graphviz: Graphviz DOT rendering and animated transitions using D3](https://github.com/magjac/d3-graphviz)\n- Python package for generating dot files: [GitHub - stricaud/gvgen: Graphviz Python dot generator](https://github.com/stricaud/gvgen)\n- The `ast` module is pretty cool, too, because with it, I can get function names with their corresponding code. Not only that, I can also get the content of for loops and if/then statements. \n# possible outline\n1. Get PyCG call graphs for the project.\n2. Create dot files from call graph.\n3. Grab function-code pairs, as well as control flow data, from ast.\n4. Include the relevant code into the functions in the dot files. \n5. Create subgraphs in the dot files, corresponding to control flow logic within the functions.\n6. Remap function calls from between function-function to between subgraph function call-function.\n7. Run functions and parts of functions through OpenAI API to get summaries, and add the summaries to the nodes in the dot file.\n8. Create visualization from the dot file with javascript package.\n9. One of two things:\n\t-  Create tree-like structure to allow for hiding/revealing of nodes, code sections, and summaries.\n\t- Somehow create on-click handlers for elements on the visualization to allow for more hands-on hiding and revealing of elements?\n# Generated project plan:\n\nYour idea sounds fantastic! Let's break down the steps to achieve this:\n\n1. **Parse the Python Codebase**: Use PyCG to parse the python codebase. This will allow you to generate the required static analysis data.\n\n2. **Generate Graph Data**: Convert the data you get from PyCG into a format that's suitable for visualization. You would likely want a node-link structure. Each function, class, or module can be a node, and the links can represent calls, inheritances, etc.\n\n3. **Visualize with Graphviz**: Use Graphviz, especially the DOT language, to create an initial visualization of the graph data. This serves as your first prototype.\n\n4. **Interactive Features**:\n    - **Zoom and Pan**: Use a tool or framework like D3.js to make the visualization interactive. D3.js allows you to zoom, pan, and manipulate the SVG elements easily.\n    - **Collapse and Hide Sections**: This can be implemented by adding interactivity where certain nodes or groups of nodes can be collapsed or expanded. Again, D3.js is perfect for this.\n    - **2D/3D Space**: If you want a 3D representation, consider using Three.js.\n\n5. **Code and Summaries**:\n    - **Displaying Code**: When a user clicks or hovers over a node, display the associated code snippet in a pop-up or a side panel.\n    - **AI-Augmented Summaries**: Use GPT-4 API to summarize the code sections. When a user interacts with a node, the AI can provide a brief, human-readable summary of what that function/class/module does.\n\n6. **Automatic Collapsing**: \n    - Use metrics such as function length, complexity, or the number of internal function calls as parameters. Functions or modules with low complexity could be collapsed by default, or you could use the GPT-4 model to predict which sections are less relevant and collapse them.\n\n7. **User Interface**: Create an intuitive UI where users can interact with the graph, navigate through it, and get insights about the codebase.\n\n8. **Continuous Updates**:\n    - **File Watcher**: Implement a file watcher to detect changes in the codebase. When a file changes, you can re-analyze and update the visualization in real-time.\n\n9. **Additional Features**:\n    - **Search \u0026 Highlight**: Allow users to search for specific functions, classes, or variables and highlight them in the graph.\n    - **Dependencies**: Show external module dependencies and how they are used in the codebase.\n\n10. **Deployment \u0026 Scaling**: Consider whether this tool will be web-based, a standalone application, or a plugin for an IDE. Also, consider scalability if you want to analyze large codebases.\n\nBuilding this would be a significant project, but the outcome could be an incredibly useful tool for developers. The combination of visualizing code relationships and having AI-powered insights could redefine the way developers explore and understand codebases. Best of luck with your project!\n\n## implementation\nI asked about how the output of pycg could be used, as well as pulling out code to send to LLMs:\n\nThe data snippet you've shared is a representation of how functions/modules (the keys) relate to other functions/modules (the values in the list) in the form of a call graph.\n\nTo focus on functions defined within a specific directory:\n\n1. **Filter by Prefix**: One approach is to use a naming convention that includes the module's path, then filter keys based on that prefix.\n\n   ```python\n   directory_prefix = \"entity_extraction_module.relationship_extraction\"\n   filtered_data = {key: value for key, value in pycg_data.items() if key.startswith(directory_prefix)}\n   ```\n\n   This way, you only retain function calls that are inside the given directory/module. However, note that if external modules/functions are called from within your target directory, they'll still appear in the lists but won't have their own keys.\n\n2. **Collect Function Definitions**: To retrieve the actual content of the functions, you'd need to use another tool or library that's able to parse the Python code and extract function definitions. A library like `ast` (Abstract Syntax Tree) that's built into Python can be quite useful for this.\n\n   Here's a simple way to grab the content of functions using the `ast` module:\n\n   ```python\n   import ast\n\n   class FunctionVisitor(ast.NodeVisitor):\n       def __init__(self):\n           self.function_data = {}\n\n       def visit_FunctionDef(self, node):\n           source_code = ast.get_source_segment(code, node)  # code is the source code of the file as a string\n           self.function_data[node.name] = source_code\n           self.generic_visit(node)\n\n   with open('your_script_path.py', 'r') as file:\n       code = file.read()\n       tree = ast.parse(code)\n       visitor = FunctionVisitor()\n       visitor.visit(tree)\n       # Now, visitor.function_data contains function names as keys and their source code as values.\n   ```\n\n   For the above to work, you need to have the entire codebase as strings. If you have a directory, loop through each file and process them one by one.\n\n3. **Integration with GPT-4**: Once you have extracted the function content, you can pass it to GPT-4 for further processing, summarization, or any other operation you're interested in.\n\nAlways remember to handle any potential exceptions or issues, like file access errors or AST parsing errors. And depending on the size of the codebase, extracting and processing function definitions might take some time, so consider implementing some caching or other optimization techniques as necessary.","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["project","python"]},"/What-sort-of-model-should-I-use":{"title":"What sort of model should I use?","content":"#work/patientsim  \n[[2024-02-07]]\n\nWell, there's lots of [[patient record model type breakdown|examples]]. I'm pretty sold on those [[Triplet Sparsity Reduction|triplets]] because they're cool and they solve the whole time+feature space sparsity issue in a pretty way. That's really more of a way of fitting the data in. Definitely superior to how it's done in the others, though. They'll:\n- cut it into windows, create overlapping embeddings\n- give each feature their own entire NN\n- stuff it into an RNN and just grab the whole final output, throw that into the main solution\n\t- or worse, they'll aggregate the RNN output by summing over the outputs of each step???? [[Merkelbach Paper|@merkelbach]]\n\t- at least one of them computes the attention or smth over the outputs of each step. idk which tho\n\nSo, it comes down to what I'm piping the triplets into. I could really pick anything. There's really two routes: \n- [[wild]] - try out the $CNN$ for time series\n- [[basic]] - just do another transformer™","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":[]},"/attention":{"title":"attention","content":"\n2024-02-07 14:38\n\nkey component of the [[transformer]]\n\nThe word embedding is fed into three layers side by side, they each make some representation of it, called query, key, and value.\n\n![[Pasted image 20240207144646.png]]\n\nthese three vectors are thrown together to get a final vector for all the words seen. That's thrown through the layers of the network to finally get a next word-prediction. \nThe throwing together is just the query of the word getting multiplied for the ","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":[]},"/basic":{"title":"basic","content":" #work/patientsim \n2024-02-07 13:59\n\nI should really go with the transformer on this one. The triplets do turn the data into a sequence of values, some with much higher relative importance than others. That feels pretty [[attention]]-native.\nThe idea is much more incremental, too. The original [[triplet transformer for patient data|triplet paper]] simply uses a self-supervised prediction objective, and I could add only the [[Variational Autoencoder|VAE]] into the mix. \n\nI mean, look at this graphic:\n![[zotero_notes/patient_sim/triplet-transformer-G7LK8EX5#^134274|triplet-transformer-G7LK8EX5]]\nLooks a whole lot like Keith's formulation of one of our core problems!\n\n","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":[]},"/coupled-rnn-paper":{"title":"coupled rnn paper","content":" #work/patientsim #paper_notes\n\n[[2024-02-05]]\n\nOne RNN gives its output to the input of the other. Each rnn gets a different overall input. This improves results when the two inputs are kinda unique, like pose info vs direct image info.\n\n[[1812.10071] Coupled Recurrent Network (CRN)](https://ar5iv.labs.arxiv.org/html/1812.10071)","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["paper_notes"]},"/decay-chat-transcript":{"title":"decay chat transcript","content":"#hebbian  \n### Me\ni wanna deep dive into just #2, the weight decay. Grab the relevant passage, repeat it back to me, then break it down. I'm no good at greek, or for that matter, math.\n\n### ChatGPT\n\nSure, let's focus on the weight decay term from the passage you provided. Here is the relevant excerpt:\n\n\"The problem with \\[the basic Hebbian learning rule\\] is that there are no mechanisms to prevent the weights from growing unbounded, thus leading to possible instability. This issue can be counteracted by adding a weight decay term γ(𝐰,𝐱) to the learning rule: Δwi=ηyxi−γ(w,x)Δwi​=ηyxi​−γ(w,x)\"\n\nNow, let's break down this concept:\n\n1. **Basic Hebbian Learning Rule Issue**: The basic Hebbian learning rule $Δw_i = η y x_i$ tends to reinforce weights whenever the input (x_i) and output (y) are correlated. However, without any constraints, this rule can cause the weights (w_i) to grow indefinitely large, leading to unstable and unpredictable behavior in the neural network.\n    \n2. **Weight Decay Term**: To address this issue, a weight decay term is introduced, denoted as γ(𝐰,𝐱). The purpose of this term is to counteract the unrestricted growth of the weights.\n    \n3. **The Updated Learning Rule**: The updated rule becomes Δw_i = η y x_i - γ(𝐰,𝐱). This means that for each weight update, instead of just reinforcing the weight based on the correlation between input and output, the rule also subtracts a value based on the weight decay term.\n    \n4. **Understanding γ(𝐰,𝐱)**: The term γ(𝐰,𝐱) is a function that depends on the current weights (𝐰) and inputs (𝐱). It's not explicitly defined in the excerpt, but generally, this function calculates a value to be subtracted from the weight update, thus preventing the weights from increasing too much.\n    \n5. **The Role of Weight Decay**: By subtracting this term, the learning rule can prevent the weights from growing too large, which helps in stabilizing the network. This decay is important for maintaining a balance in the network, ensuring that it learns effectively without its weights \"exploding\" or becoming too influential.\n    \n\nIn simple terms, think of the weight decay term as a \"moderator\" in the learning process. While the Hebbian rule strengthens the connection based on input-output correlations, the weight decay term ensures that this strengthening doesn't go overboard, keeping the network's learning process stable and grounded.\n\n### Me\n\noh, it's not defined. I thought it might be a hyperparameter. seems like this actually bleeds into the competitive learning rule a bit\n\n### ChatGPT\nYes, you are correct. The weight decay term, represented by γ(𝐰,𝐱), is indeed not explicitly defined in the excerpt you provided. This term typically serves as a mechanism to control or regularize the weight changes in the network, and it can take various forms depending on the specific implementation or the desired characteristics of the learning model.\n\nIn the context of your excerpt, this weight decay term does indeed bleed into the concept of competitive learning, as shown in the following formula from your text:\n\n$$ \\Delta w_i = \\eta y x_i - \\eta y w_i = \\eta y (x_i - w_i) $$\n\nThis formula represents a competitive learning rule. Here's a breakdown:\n\n1. **Competitive Aspect**: The term $- \\eta y w_i$ introduces competition among the weights. In competitive learning, neurons in a layer compete with each other to respond to a subset of the input patterns. This competition often leads to specialization among the neurons, with different neurons learning to respond strongly to different inputs.\n\n2. **Stabilizing Weights**: The $- \\eta y w_i$ term acts as a stabilizing factor, preventing any single weight from growing too large without bound, which aligns with the purpose of a weight decay term.\n\n3. **Balancing Act**: The full equation $\\Delta w_i = \\eta y (x_i - w_i)$ balances two aspects: reinforcing weights when the input and output are correlated (via $\\eta y x_i$), and applying a competitive constraint that discourages runaway growth of weights (via $- \\eta y w_i$).\n\nIn summary, while the weight decay term in the context of the Hebbian learning rule is not explicitly defined, its role is crucial for maintaining stability in the learning process. When it bleeds into the competitive learning rule, it serves a dual purpose: reinforcing relevant connections while preventing any single connection from dominating due to excessive strengthening. This approach leads to a more balanced and stable learning dynamic in neural networks.\n\n### Me\n\nDo *you* know of any other formulations for the decay term? otherwise, I oughta stick to the competitive one.\n\n### ChatGPT\n\nYes, there are several formulations for the weight decay term that are commonly used in neural network models. Depending on your specific requirements and the nature of your model, you might consider one of the following alternatives:\n\n1. **L2 Regularization (Ridge Regression)**: ^4045d3\n   - The most common form of weight decay is L2 regularization, where the decay term is proportional to the square of the weights:\n     $$ \\gamma(\\mathbf{w}) = \\lambda \\|\\mathbf{w}\\|^2 $$\n   - Here, λ (lambda) is a hyperparameter that controls the strength of the regularization. This method penalizes larger weights more heavily, encouraging the model to develop smaller, more distributed weights, which can prevent overfitting.\n\n2. **L1 Regularization (Lasso Regression)**:\n   - Another approach is L1 regularization, where the decay term is proportional to the absolute value of the weights:\n     $$ \\gamma(\\mathbf{w}) = \\lambda \\|\\mathbf{w}\\|_1 $$\n   - This method encourages sparsity in the weight matrix, which can be beneficial if you believe that only a small number of features should contribute significantly to the output.\n\n3. **Elastic Net Regularization**:\n   - Elastic Net is a combination of L1 and L2 regularization:\n     $$ \\gamma(\\mathbf{w}) = \\lambda_1 \\|\\mathbf{w}\\|^2 + \\lambda_2 \\|\\mathbf{w}\\|_1 $$\n   - This approach tries to balance between the feature selection of L1 and the smooth regularization of L2.\n\n4. **Weight Normalization**:\n   - Instead of penalizing large weights, some models normalize the weights during each update:\n     $$ \\mathbf{w} \\leftarrow \\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|} $$\n   - This keeps the magnitude of the weights fixed, while allowing their direction to change.\n\n5. **Custom Decay Functions**:\n   - Depending on the specific needs of your model, you might design a custom decay function that applies penalties based on certain criteria, like the age of the weight (how long it's been since it was last updated), the variance of the weight over time, or other domain-specific factors.\n\nThe choice of the weight decay formulation can significantly impact the behavior and performance of your neural network. The competitive learning rule that you mentioned earlier is indeed a form of weight control, but these additional methods provide alternative ways to manage weight magnitudes, each with its own advantages and use-cases. Experimentation and testing are essential to determine which method aligns best with your model's objectives and the nature of your data.","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["hebbian"]},"/deep-dive-into-weight-decay-term":{"title":"deep dive into weight decay term","content":" #hebbian \n\nthe term: ![[List of Hebbian Weight Update Rules#Weight Decay Term Added]]\n\n\u003e The problem with \\[the basic Hebbian learning rule\\] is that there are no mechanisms to prevent the weights from growing unbounded, thus leading to possible instability. This issue can be counteracted by adding a weight decay term γ(𝐰,𝐱) to the learning rule\n\n\nThe paper doesn't define any specific weight decay term besides the [[List of Hebbian Weight Update Rules#Competitive Learning Rule|Competitive Learning Rule]], but there could be others. \n- L2 Regularization\n- L1 Regularization\n- Elastic Net Regularization\n- Weight Normalization\n\nI'm already doing Weight Normalization, with the L2 norm, here:\n![[Current Build of the Memory Encoding Hebbian RNN#^99b36f]]\n However, the Ridge Regression (L2 again) [[decay chat transcript#^4045d3|here]] is typically not applied to the weights by division, but instead as a decay term to the weight update, so like this:\n \n$$ \\Delta w_i = \\eta y x_i - \\gamma(\\mathbf{w}, \\mathbf{x}) $$\n$$\\gamma(\\mathbf{w}, \\mathbf{x}) = \\gamma(\\mathbf{w}) = \\lambda \\|\\mathbf{w}\\|^2$$\ni guess we are just not caring about $\\mathbf{x}$ anymore, which is mentally healthy behavior and should be encouraged.\nfinally,\n$$\\Delta w_i = \\eta y x_i - \\lambda \\|\\mathbf{w}\\|^2$$\n\n, I guess?\n\ndetermined from [[decay chat transcript]]","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["hebbian","Weight","Competitive"]},"/dr-fulda-call-notes-feb-2024":{"title":"dr fulda call notes feb 2024","content":" #hebbian #phd  \n2024-02-13 12:54\n\n- consider a custom dataset, specifically built to prioritize learning new info fast. It would have a defined sequence, with related information grouped together in time. If project gutenberg, for example, load from one book until finished, then move on to next. \n- perhaps that [[Cyclic Plasticity Values]] idea would be good at this, letting us see exactly which neurons are used to encode a certain sort of data. \n- put in your own ticket for vpn and research computing access. \n- maybe you start in the fall?\n\n\nwood chiang\ncarl warnick\n\njordan yorgason\n","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["hebbian","phd"]},"/evolve_and_merge":{"title":"evolve_and_merge","content":" #hebbian #phd  \n2024-02-12 07:13\n\n[[List of Hebbian Weight Update Rules#ABCD|ABCD]], again. Just the same as [[zotero_notes/hebbian/meta-learning-through-hebbian-plasticity-KG2BPTBB#^5c0f88|meta-learning-through-hebbian-plasticity-KG2BPTBB]]\nThe point in this one was to reduce the number of rules while also increasing generalizability. Simply superior.\n\nIt's cool how they are very indirectly training the network. The parameters aren't really the weights, but the plasticity parameters. Very neat.\n\n![[zotero_notes/hebbian/evolveandmerge-HNAI4JV6|evolveandmerge-HNAI4JV6]]","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["hebbian","phd"]},"/evolving-neuromodulatory-topologies":{"title":"evolving neuromodulatory topologies","content":" #hebbian #phd  \n2024-02-12 07:19\n\n[[List of Hebbian Weight Update Rules#ABCD|ABCD]] once more, this time with genetically decided topology. Topology, in this case, meaning number of neurons and connections. \n\nIt's better described with a picture:\n![[zotero_notes/hebbian/evolving-neuromodulatory-topologies-MFUVKEVY|evolving-neuromodulatory-topologies-MFUVKEVY]]","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["hebbian","phd"]},"/meta-learning-with-hospital-data":{"title":"meta-learning with hospital data","content":" #work/patientsim #paper_notes\n[[2024-02-05]]\n\nDynEHR, written about in 2021, was frustrating to figure out, due to a paywall. I just grabbed its description from a paper that speaks about it:\n\u003e DynEHR, a model designed to adapt to various ICU lengths of stay. This approach employs an LSTM model on the time-series data and applies MAML on the top of the LSTM layers. The performance of DynEHR was evaluated on the MIMIC-III dataset for different tasks.\n\nUnderwhelming, given the effort I put in to get it. \n\nThe second paper is more useful+available. It seems to suggest that the biggest benefits of meta-learning are concerned with there being a very low amount of data available in a new domain. Seems cool, but it's not the problem I'm dealing with. \n\n[[2308.02877] Meta-learning in healthcare: A survey](https://ar5iv.labs.arxiv.org/html/2308.02877)","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["paper_notes"]},"/model-agnostic-metalearning":{"title":"model-agnostic metalearning","content":"#work/patientsim  \n\n[[1703.03400] Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://ar5iv.labs.arxiv.org/html/1703.03400)\n\nKinda out-there. Worth further investigating. Popular enough to find stuff on youtube.","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":[]},"/notes-on-patient-similarity":{"title":"notes on patient similarity","content":"#work/patientsim \n\n# obvious structure\n- denoising autoencoders seem like the clear way to go for making some sort of embedding/latent space for patients\n- the denoising is interesting for discussion, especially if we have additional boolean variables signifying whether the input is even present. When comes down to it, should a patient's data be represented identically if info is omitted? Are two patients the same if one has data the other doesn't? Keith said that they may not be, since missing data is information, in itself. If that's the case, we shouldn't do denoising. \n\n# time series\nThis one is weird, because it may truly just be a matter of using a mix of existing architectures.\n- some papers used RNNs like GRU, LSTM, but [[Transformer-based Solutions|transformer]] is king, when the context length is small enough. [[patient record model type breakdown]]\n- Context length is likely a limiting factor, so I can understand that only clustered bits were given little self-attention blocks. This is akin to a lot of hierarchical solutions to the context length problem found in plenty of big new papers. \n\n# conceptualizing\nGeneral vibes.\n\n- really, isn't what the end goal is to just identify truly similar patients? We're looking for a latent space where the similar patients are closer than dissimilar. \n- To do that, we need something that weighs inputs by significance. Does autoencoding actually do that?? Really, how do we tell it that gender is a bigger deal than the material used in the dressing? Does it figure it out by how it affects other values?? I think the model needs an actual objective before this weighing can occur. \n\n# out-there brainstorming\n![[ideas about ml structure for healthcare]]\n# reading log\n\n![[Patient Similarity Readings Log]]","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":[]},"/patient-record-model-type-breakdown":{"title":"patient record model type breakdown","content":"#work/patientsim  \n[[2024-02-06]]\n## transformer\n![[Transformer-based Solutions]]\n\n## RNNs\n- [[meta-learning with hospital data|DynEHR]], metalearning with [[model-agnostic metalearning|MAML]] atop an LSTM\n- [[coupled rnn paper|coupled rnns]] well-suited to dual data types, like image+poses\n- [[Generating Synthetic Data Paper|EHR-M-GAN]] is based on an LSTM for its encoding of time series, which is based off [DRAW](https://arxiv.org/pdf/1502.04623.pdf).\n- [[Merkelbach Paper]] just uses a GRU, pretty basic. One of the few that demonstrate the true autoencoder structure I'm looking to use any of this stuff for, though. Everything else just uses some sort of outcomes or next-event prediction or smth.\n\n\n## Convolutional Neural Networks (CNNs)\n- [[PatchMixer|PatchMixer]], very strange. It compares itself a **lot** with the [[transformer]], though.\n- [[zotero_notes/patient_sim/dilated-convolutions-BC5HWKRF|dilated-convolutions-BC5HWKRF]] very weird","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":[]},"/patient-similarity-model":{"title":"patient similarity model","content":"#work/patientsim  \n\n# choice\nI've chosen [[basic]]. With some modifications, of course. \nI'm creating an [[autoencoder]], so there needs to be a way to both ingest and reconstruct the time series input. That's not exactly a solved problem.\n\nI'll use transformers on both sides, the [[Triplet Sparsity Reduction|triplets]] treated as tokens.\n1. I'll sum the outputs on a final attention layer to create the single-vector patient representation vector. \n2. The second transformer is a next-token predictor. Structured as a typical decoder-only transformer like GPT. During training, it receives preceding tokens directly from the same training data as the encoding transformer. However, it also gets the patient representation vector concatted in beside the attention layer. \n3. An additional value might be included with the patient representation vector, to inform the transformer what data type is being asked for.\n\nOf course the decoding transformer could be trained on next-token prediction alone, but the idea is that we can create a cheat sheet for the whole patient that makes the task trivial. Ideally, the encoder makes any real prediction unnecessary. \n\nThe idea is kinda similar to [[stable diffusion]], in that a model is trained to reconstruct incomplete data, based on some conditioning input. Except, in my case, the focus is less on the reconstruction ability, and more on the fidelity of the conditioning input. \nIn stable diffusion, the autoencoder is actually frozen! All the action is happening inside the latent space. \n\nIt's barely an autoencoder, really. A single training instance is not asking the decoder to reconstruct the full input, even, but just a small piece. ","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":[]},"/private/notes/CJK-+-Latex-Support-%E6%B5%8B%E8%AF%95":{"title":"CJK + Latex Support (测试)","content":"\n## Chinese, Japanese, Korean Support\n几乎在我们意识到之前，我们已经离开了地面。\n\n우리가 그것을 알기도 전에 우리는 땅을 떠났습니다.\n\n私たちがそれを知るほぼ前に、私たちは地面を離れていました。\n\n## Latex\n\nBlock math works with two dollar signs `$$...$$`\n\n$$f(x) = \\int_{-\\infty}^\\infty\n    f\\hat(\\xi),e^{2 \\pi i \\xi x}\n    \\,d\\xi$$\n\t\nInline math also works with single dollar signs `$...$`. For example, Euler's identity but inline: $e^{i\\pi} = -1$\n\nAligned equations work quite well:\n\n$$\n\\begin{aligned}\na \u0026= b + c \\\\ \u0026= e + f \\\\\n\\end{aligned}\n$$\n\nAnd matrices\n\n$$\n\\begin{bmatrix}\n1 \u0026 2 \u0026 3 \\\\\na \u0026 b \u0026 c\n\\end{bmatrix}\n$$\n\n## RTL\nMore information on configuring RTL languages like Arabic in the [config](notes/config.md) page.\n","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":[]},"/private/notes/callouts":{"title":"Callouts","content":"\n## Callout support\n\nQuartz supports the same Admonition-callout syntax as Obsidian.\n\nThis includes\n- 12 Distinct callout types (each with several aliases)\n- Collapsable callouts\n\nSee [documentation on supported types and syntax here](https://help.obsidian.md/Editing+and+formatting/Callouts).\n\n## Showcase\n\n\u003e [!EXAMPLE] Examples\n\u003e\n\u003e Aliases: example\n\n\u003e [!note] Notes\n\u003e\n\u003e Aliases: note\n\n\u003e [!abstract] Summaries \n\u003e\n\u003e Aliases: abstract, summary, tldr\n\n\u003e [!info] Info \n\u003e\n\u003e Aliases: info, todo\n\n\u003e [!tip] Hint \n\u003e\n\u003e Aliases: tip, hint, important\n\n\u003e [!success] Success \n\u003e\n\u003e Aliases: success, check, done\n\n\u003e [!question] Question \n\u003e\n\u003e Aliases: question, help, faq\n\n\u003e [!warning] Warning \n\u003e\n\u003e Aliases: warning, caution, attention\n\n\u003e [!failure] Failure \n\u003e\n\u003e Aliases: failure, fail, missing\n\n\u003e [!danger] Error\n\u003e\n\u003e Aliases: danger, error\n\n\u003e [!bug] Bug\n\u003e\n\u003e Aliases: bug\n\n\u003e [!quote] Quote\n\u003e\n\u003e Aliases: quote, cite\n","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":[]},"/private/notes/config":{"title":"Configuration","content":"\n## Configuration\nQuartz is designed to be extremely configurable. You can find the bulk of the configuration scattered throughout the repository depending on how in-depth you'd like to get.\n\nThe majority of configuration can be found under `data/config.yaml`. An annotated example configuration is shown below.\n\n```yaml {title=\"data/config.yaml\"}\n# The name to display in the footer\nname: Jacky Zhao\n\n# whether to globally show the table of contents on each page\n# this can be turned off on a per-page basis by adding this to the\n# front-matter of that note\nenableToc: true\n\n# whether to by-default open or close the table of contents on each page\nopenToc: false\n\n# whether to display on-hover link preview cards\nenableLinkPreview: true\n\n# whether to render titles for code blocks\nenableCodeBlockTitle: true \n\n# whether to render copy buttons for code blocks\nenableCodeBlockCopy: true \n\n# whether to render callouts\nenableCallouts: true\n\n# whether to try to process Latex\nenableLatex: true\n\n# whether to enable single-page-app style rendering\n# this prevents flashes of unstyled content and improves\n# smoothness of Quartz. More info in issue #109 on GitHub\nenableSPA: true\n\n# whether to render a footer\nenableFooter: true\n\n# whether backlinks of pages should show the context in which\n# they were mentioned\nenableContextualBacklinks: true\n\n# whether to show a section of recent notes on the home page\nenableRecentNotes: false\n\n# whether to display an 'edit' button next to the last edited field\n# that links to github\nenableGitHubEdit: true\nGitHubLink: https://github.com/jackyzha0/quartz/tree/hugo/content\n\n# whether to render mermaid diagrams\nenableMermaid: true\n\n# whether to use Operand to power semantic search\n# IMPORTANT: replace this API key with your own if you plan on using\n# Operand search!\nsearch:\n  enableSemanticSearch: false\n  operandApiKey: \"REPLACE-WITH-YOUR-OPERAND-API-KEY\"\n  operandIndexId: \"REPLACE-WITH-YOUR-OPERAND-INDEX-ID\"\n\n# page description used for SEO\ndescription:\n  Host your second brain and digital garden for free. Quartz features extremely fast full-text search,\n  Wikilink support, backlinks, local graph, tags, and link previews.\n\n# title of the home page (also for SEO)\npage_title:\n  \"🪴 Quartz 3.3\"\n\n# links to show in the footer\nlinks:\n  - link_name: Twitter\n    link: https://twitter.com/_jzhao\n  - link_name: Github\n    link: https://github.com/jackyzha0\n```\n\n### Code Block Titles\nTo add code block titles with Quartz:\n\n1. Ensure that code block titles are enabled in Quartz's configuration:\n\n    ```yaml {title=\"data/config.yaml\", linenos=false}\n    enableCodeBlockTitle: true\n    ```\n\n2. Add the `title` attribute to the desired [code block\n   fence](https://gohugo.io/content-management/syntax-highlighting/#highlighting-in-code-fences):\n\n      ```markdown {linenos=false}\n       ```yaml {title=\"data/config.yaml\"}\n       enableCodeBlockTitle: true  # example from step 1\n       ```\n      ```\n\n**Note** that if `{title=\u003cmy-title\u003e}` is included, and code block titles are not\nenabled, no errors will occur, and the title attribute will be ignored.\n\n### HTML Favicons\nIf you would like to customize the favicons of your Quartz-based website, you \ncan add them to the `data/config.yaml` file. The **default** without any set \n`favicon` key is:\n\n```html {title=\"layouts/partials/head.html\", linenostart=15}\n\u003clink rel=\"shortcut icon\" href=\"icon.png\" type=\"image/png\"\u003e\n```\n\nThe default can be overridden by defining a value to the `favicon` key in your \n`data/config.yaml` file. For example, here is a `List[Dictionary]` example format, which is\nequivalent to the default:\n\n```yaml {title=\"data/config.yaml\", linenos=false}\nfavicon:\n  - { rel: \"shortcut icon\", href: \"icon.png\", type: \"image/png\" }\n#  - { ... } # Repeat for each additional favicon you want to add\n```\n\nIn this format, the keys are identical to their HTML representations.\n\nIf you plan to add multiple favicons generated by a website (see list below), it\nmay be easier to define it as HTML. Here is an example which appends the \n**Apple touch icon** to Quartz's default favicon:\n\n```yaml {title=\"data/config.yaml\", linenos=false}\nfavicon: |\n  \u003clink rel=\"shortcut icon\" href=\"icon.png\" type=\"image/png\"\u003e\n  \u003clink rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"/apple-touch-icon.png\"\u003e\n```\n\nThis second favicon will now be used as a web page icon when someone adds your \nwebpage to the home screen of their Apple device. If you are interested in more \ninformation about the current and past standards of favicons, you can read \n[this article](https://www.emergeinteractive.com/insights/detail/the-essentials-of-favicons/).\n\n**Note** that all generated favicon paths, defined by the `href` \nattribute, are relative to the `static/` directory.\n\n### Graph View\nTo customize the Interactive Graph view, you can poke around `data/graphConfig.yaml`.\n\n```yaml {title=\"data/graphConfig.yaml\"}\n# if true, a Global Graph will be shown on home page with full width, no backlink.\n# A different set of Local Graphs will be shown on sub pages.\n# if false, Local Graph will be default on every page as usual\nenableGlobalGraph: false\n\n### Local Graph ###\nlocalGraph:\n    # whether automatically generate a legend\n    enableLegend: false\n    \n    # whether to allow dragging nodes in the graph\n    enableDrag: true\n    \n    # whether to allow zooming and panning the graph\n    enableZoom: true\n    \n    # how many neighbours of the current node to show (-1 is all nodes)\n    depth: 1\n    \n    # initial zoom factor of the graph\n    scale: 1.2\n    \n    # how strongly nodes should repel each other\n    repelForce: 2\n\n    # how strongly should nodes be attracted to the center of gravity\n    centerForce: 1\n\n    # what the default link length should be\n    linkDistance: 1\n    \n    # how big the node labels should be\n    fontSize: 0.6\n    \n    # scale at which to start fading the labes on nodes\n    opacityScale: 3\n\n### Global Graph ###\nglobalGraph:\n\t# same settings as above\n\n### For all graphs ###\n# colour specific nodes path off of their path\npaths:\n  - /moc: \"#4388cc\"\n```\n\n\n## Styling\nWant to go even more in-depth? You can add custom CSS styling and change existing colours through editing `assets/styles/custom.scss`. If you'd like to target specific parts of the site, you can add ids and classes to the HTML partials in `/layouts/partials`. \n\n### Partials\nPartials are what dictate what gets rendered to the page. Want to change how pages are styled and structured? You can edit the appropriate layout in `/layouts`.\n\nFor example, the structure of the home page can be edited through `/layouts/index.html`. To customize the footer, you can edit `/layouts/partials/footer.html`\n\nMore info about partials on [Hugo's website.](https://gohugo.io/templates/partials/)\n\nStill having problems? Checkout our [FAQ and Troubleshooting guide](notes/troubleshooting.md).\n\n## Language Support\n[CJK + Latex Support (测试)](notes/CJK%20+%20Latex%20Support%20(测试).md) comes out of the box with Quartz.\n\nWant to support languages that read from right-to-left (like Arabic)? Hugo (and by proxy, Quartz) supports this natively.\n\nFollow the steps [Hugo provides here](https://gohugo.io/content-management/multilingual/#configure-languages) and modify your `config.toml`\n\nFor example:\n\n```toml\ndefaultContentLanguage = 'ar'\n[languages]\n  [languages.ar]\n    languagedirection = 'rtl'\n    title = 'مدونتي'\n    weight = 1\n```\n","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["setup"]},"/private/notes/custom-Domain":{"title":"Custom Domain","content":"\n### Registrar\nThis step is only applicable if you are using a **custom domain**! If you are using a `\u003cYOUR-USERNAME\u003e.github.io` domain, you can skip this step.\n\nFor this last bit to take effect, you also need to create a CNAME record with the DNS provider you register your domain with (i.e. NameCheap, Google Domains).\n\nGitHub has some [documentation on this](https://docs.github.com/en/pages/configuring-a-custom-domain-for-your-github-pages-site/managing-a-custom-domain-for-your-github-pages-site), but the tldr; is to\n\n1. Go to your forked repository (`github.com/\u003cYOUR-GITHUB-USERNAME\u003e/quartz`) settings page and go to the Pages tab. Under \"Custom domain\", type your custom domain, then click **Save**.\n2. Go to your DNS Provider and create a CNAME record that points from your domain to `\u003cYOUR-GITHUB-USERNAME.github.io.` (yes, with the trailing period).\n\n\t![Example Configuration for Quartz](/notes/images/google-domains.png)*Example Configuration for Quartz*\n3. Wait 30 minutes to an hour for the network changes to kick in.\n4. Done!","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":[]},"/private/notes/docker":{"title":"Hosting with Docker","content":"\nIf you want to host Quartz on a machine without using a webpage hosting service, it may be easier to [install Docker Compose](https://docs.docker.com/compose/install/) and follow the instructions below than to [install Quartz's dependencies manually](notes/preview%20changes.md).\n## Hosting Quartz Locally\nYou can serve Quartz locally at `http://localhost:1313` with the following script, replacing `/path/to/quartz` with the \nactual path to your Quartz folder.\n\ndocker-compose.yml\n```\nservices:\n  quartz-hugo:\n    image: ghcr.io/jackyzha0/quartz:hugo\n    container_name: quartz-hugo\n    volumes:\n      - /path/to/quartz:/quartz\n    ports:\n      - 1313:1313\n\n    # optional\n    environment:\n      - HUGO_BIND=0.0.0.0\n      - HUGO_BASEURL=http://localhost\n      - HUGO_PORT=1313\n      - HUGO_APPENDPORT=true\n      - HUGO_LIVERELOADPORT=-1\n```\n\nThen run with: `docker-compose up -d` in the same directory as your `docker-compose.yml` file.\n\nWhile the container is running, you can update the `quartz` fork with: `docker exec -it quartz-hugo make update`.\n\n## Exposing Your Container to the Internet\n\n### To Your Public IP Address with Port Forwarding (insecure)\n\nAssuming you are already familiar with [port forwarding](https://en.wikipedia.org/wiki/Port_forwarding) and [setting it up with your router model](https://portforward.com):\n\n1. You should set the environment variable `HUGO_BASEURL=http://your-public-ip` and then start your container.\n2. Set up port forwarding on your router from port `p` to `your-local-ip:1313`.\n3. You should now be able to access Quartz from outside your local network at `http://your-public-ip:p`.\n\nHowever, your HTTP connection will be unencrypted and **this method is not secure**.\n\n### To a Domain using Cloudflare Proxy\n\n1. Port forward 443 (HTTPS) from your machine.\n2. Buy a custom domain (say, `your-domain.com`) from [Cloudflare](https://www.cloudflare.com/products/registrar/). Point a DNS A record from `your-domain.com` to your public IP address and enable the proxy.\n3. Set the environment variables `HUGO_BASEURL=https://your-domain.com`, `HUGO_PORT=443`, and `HUGO_APPENDPORT=false`. Change `1313:1313` to `443:443` for the `ports` in `docker-compose.yml`.\n4. Spin up your Quartz container and enjoy it at `https://your-domain.com`!\n\n### To a Domain using a Reverse Proxy\n\nIf you want to serve more than just Quartz to the internet on this machine (or don't want to use the Cloudflare registrar and proxy), you should follow the steps in the section above (as appropriate) and also set up a reverse proxy, like [Traefik](https://doc.traefik.io/traefik). Be sure to configure your TLS certificates too!\n","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["setup"]},"/private/notes/editing":{"title":"Editing Content in Quartz","content":"\n## Editing \nQuartz runs on top of [Hugo](https://gohugo.io/) so all notes are written in [Markdown](https://www.markdownguide.org/getting-started/).\n\n### Folder Structure\nHere's a rough overview of what's what.\n\n**All content in your garden can found in the `/content` folder.** To make edits, you can open any of the files and make changes directly and save it. You can organize content into any folder you'd like.\n\n**To edit the main home page, open `/content/_index.md`.**\n\n### Front Matter\nHugo is picky when it comes to metadata for files. Make sure that your title is double-quoted and that you have a title defined at the top of your file like so, otherwise the generated page will not have a title!\n\nYou can also add tags here as well.\n\n```yaml\n---\ntitle: \"Example Title\"\ntags:\n- example-tag\n---\n\nRest of your content here...\n```\n\n### Obsidian\nI recommend using [Obsidian](http://obsidian.md/) as a way to edit and grow your digital garden. It comes with a really nice editor and graphical interface to preview all of your local files.\n\nThis step is **highly recommended**.\n\n\u003e 🔗 Step 3: [How to setup your Obsidian Vault to work with Quartz](notes/obsidian.md)\n\n## Previewing Changes\nThis step is purely optional and mostly for those who want to see the published version of their digital garden locally before opening it up to the internet. This is *highly recommended* but not required.\n\n\u003e 👀 Step 4: [Preview Quartz Changes](notes/preview%20changes.md)\n\nFor those who like to live life more on the edge, viewing the garden through Obsidian gets you pretty close to the real thing.\n\n## Publishing Changes\nNow that you know the basics of managing your digital garden using Quartz, you can publish it to the internet!\n\n\u003e 🌍 Step 5: [Hosting Quartz online!](notes/hosting.md)\n\nHaving problems? Checkout our [FAQ and Troubleshooting guide](notes/troubleshooting.md).\n","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["setup"]},"/private/notes/hosting":{"title":"Deploying Quartz to the Web","content":"\n## Hosting on GitHub Pages\nQuartz is designed to be effortless to deploy. If you forked and cloned Quartz directly from the repository, everything should already be good to go! Follow the steps below.\n\n### Enable GitHub Actions Permissions\nBy default, GitHub disables workflows from modifying your files (for good reason!). However, Quartz needs this to write the actual site files back to GitHub.\n\nHead to `Settings \u003e Action \u003e General \u003e Workflow Permissions` and choose `Read and Write Permissions`\n\n![[notes/images/github-actions.png]]\n*Enable GitHub Actions*\n\n### Enable GitHub Pages\n\nHead to the 'Settings' tab of your forked repository and go to the 'Pages' tab.\n\n1. (IMPORTANT) Set the source to deploy from `master` (and not `hugo`) using `/ (root)`\n2. Set a custom domain here if you have one!\n\n![Enable GitHub Pages](/notes/images/github-pages.png)*Enable GitHub Pages*\n\n### Pushing Changes\nTo see your changes on the internet, we need to push it them to GitHub. Quartz is a `git` repository so updating it is the same workflow as you would follow as if it were just a regular software project.\n\n```shell\n# Navigate to Quartz folder\ncd \u003cpath-to-quartz\u003e\n\n# Commit all changes\ngit add .\ngit commit -m \"message describing changes\"\n\n# Push to GitHub to update site\ngit push origin hugo\n```\n\nNote: we specifically push to the `hugo` branch here. Our GitHub action automatically runs everytime a push to is detected to that branch and then updates the `master` branch for redeployment.\n\n### Setting up the Site\nNow let's get this site up and running. Never hosted a site before? No problem. Have a fancy custom domain you already own or want to subdomain your Quartz? That's easy too.\n\nHere, we take advantage of GitHub's free page hosting to deploy our site. Change `baseURL` in `/config.toml`. \n\nMake sure that your `baseURL` has a trailing `/`!\n\n[Reference `config.toml` here](https://github.com/jackyzha0/quartz/blob/hugo/config.toml)\n\n```toml\nbaseURL = \"https://\u003cYOUR-DOMAIN\u003e/\"\n```\n\nIf you are using this under a subdomain (e.g. `\u003cYOUR-GITHUB-USERNAME\u003e.github.io/quartz`), include the trailing `/`. **You need to do this especially if you are using GitHub!**\n\n```toml\nbaseURL = \"https://\u003cYOUR-GITHUB-USERNAME\u003e.github.io/quartz/\"\n```\n\nChange `cname` in `/.github/workflows/deploy.yaml`. Again, if you don't have a custom domain to use, you can use `\u003cYOUR-USERNAME\u003e.github.io`.\n\nPlease note that the `cname` field should *not* have any path `e.g. end with /quartz` or have a trailing `/`.\n\n[Reference `deploy.yaml` here](https://github.com/jackyzha0/quartz/blob/hugo/.github/workflows/deploy.yaml)\n\n```yaml {title=\".github/workflows/deploy.yaml\"}\n- name: Deploy  \n  uses: peaceiris/actions-gh-pages@v3  \n  with:  \n\tgithub_token: ${{ secrets.GITHUB_TOKEN }} # this can stay as is, GitHub fills this in for us!\n\tpublish_dir: ./public  \n\tpublish_branch: master\n\tcname: \u003cYOUR-DOMAIN\u003e\n```\n\nHave a custom domain? [Learn how to set it up with Quartz ](notes/custom%20Domain.md).\n\n### Ignoring Files\nOnly want to publish a subset of all of your notes? Don't worry, Quartz makes this a simple two-step process.\n\n❌ [Excluding pages from being published](notes/ignore%20notes.md)\n\n## Docker Support\nIf you don't want to use a hosting service, you can host using [Docker](notes/docker.md) instead!\nI would *not use this method* unless you know what you are doing.\n\n---\n\nNow that your Quartz is live, let's figure out how to make Quartz really *yours*!\n\n\u003e Step 6: 🎨 [Customizing Quartz](notes/config.md)\n\nHaving problems? Checkout our [FAQ and Troubleshooting guide](notes/troubleshooting.md).\n","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":["setup"]},"/private/notes/ignore-notes":{"title":"Ignoring Notes","content":"\n### Quartz Ignore\nEdit `ignoreFiles` in `config.toml` to include paths you'd like to exclude from being rendered.\n\n```toml\n...\nignoreFiles = [  \n    \"/content/templates/*\",  \n    \"/content/private/*\", \n    \"\u003cyour path here\u003e\"\n]\n```\n\n`ignoreFiles` supports the use of Regular Expressions (RegEx) so you can ignore patterns as well (e.g. ignoring all `.png`s by doing `\\\\.png$`).\nTo ignore a specific file, you can also add the tag `draft: true` to the frontmatter of a note.\n\n```markdown\n---\ntitle: Some Private Note\ndraft: true\n---\n...\n```\n\nMore details in [Hugo's documentation](https://gohugo.io/getting-started/configuration/#ignore-content-and-data-files-when-rendering).\n\n### Global Ignore\nHowever, just adding to the `ignoreFiles` will only prevent the page from being access through Quartz. If you want to prevent the file from being pushed to GitHub (for example if you have a public repository), you need to also add the path to the `.gitignore` file at the root of the repository.","lastmodified":"2024-02-27T19:19:42.827947777Z","tags":[]},"/private/notes/obsidian":{"title":"Obsidian Vault Integration","content":"\n## Setup\nObsidian is the preferred way to use Quartz. You can either create a new Obsidian Vault or link one that your already have.\n\n### New Vault\nIf you don't have an existing Vault, [download Obsidian](https://obsidian.md/) and create a new Vault in the `/content` folder that you created and cloned during the [setup](notes/setup.md) step.\n\n### Linking an existing Vault\nThe easiest way to use an existing Vault is to copy all of your files (directory and hierarchies intact) into the `/content` folder.\n\n## Settings\nGreat, now that you have your Obsidian linked to your Quartz, let's fix some settings so that they play well.\n\nOpen Settings \u003e Files \u0026 Links and look for these two items:\n\n1. Set the **New link format** to **Absolute Path in vault**. If you have a completely flat vault (no folders), this step isn't necessary.\n2. Turn **on** the **Automatically update internal links** setting.\n\n\n![[notes/images/obsidian-settings.png]]*Obsidian Settings*\n\n## Templates\nInserting front matter everytime you want to create a new Note gets annoying really quickly. Luckily, Obsidian supports templates which makes inserting new content really easily.\n\n\u003e [!WARNING]\n\u003e \n\u003e **If you decide to overwrite the `/content` folder completely, don't remove the `/content/templates` folder!**\n\nHead over to Options \u003e Core Plugins and enable the Templates plugin. Then go to Options \u003e Hotkeys and set a hotkey for 'Insert Template' (I recommend `[cmd]+T`). That way, when you create a new note, you can just press the hotkey for a new template and be ready to go!\n\n\u003e 👀 Step 4: [Preview Quartz Changes](notes/preview%20changes.md)\n","lastmodified":"2024-02-27T19:19:42.831947794Z","tags":["setup"]},"/private/notes/philosophy":{"title":"Quartz Philosophy","content":"\n\u003e “[One] who works with the door open gets all kinds of interruptions, but [they] also occasionally gets clues as to what the world is and what might be important.” — Richard Hamming\n\n## Why Quartz?\nHosting a public digital garden isn't easy. There are an overwhelming number of tutorials, resources, and guides for tools like [Notion](https://www.notion.so/), [Roam](https://roamresearch.com/), and [Obsidian](https://obsidian.md/), yet none of them have super easy to use *free* tools to publish that garden to the world.\n\nI've personally found that\n1. It's nice to access notes from anywhere\n2. Having a public digital garden invites open conversations\n3. It makes keeping personal notes and knowledge *playful and fun*\n\nI was really inspired by [Bianca](https://garden.bianca.digital/) and [Joel](https://joelhooks.com/digital-garden)'s digital gardens and wanted to try making my own.\n\n**The goal of Quartz is to make hosting your own public digital garden free and simple.** You don't even need your own website. Quartz does all of that for you and gives your own little corner of the internet.\n","lastmodified":"2024-02-27T19:19:42.831947794Z","tags":[]},"/private/notes/preview-changes":{"title":"Preview Changes","content":"\nIf you'd like to preview what your Quartz site looks like before deploying it to the internet, the following\ninstructions guide you through installing the proper dependencies to run it locally.\n\n\n## Install `hugo-obsidian`\nThis step will generate the list of backlinks for Hugo to parse. Ensure you have [Go](https://golang.org/doc/install) (\u003e= 1.16) installed.\n\n```bash\n# Install and link `hugo-obsidian` locally\ngo install github.com/jackyzha0/hugo-obsidian@latest\n```\n\nIf you are running into an error saying that `command not found: hugo-obsidian`, make sure you set your `GOPATH` correctly (see [[notes/troubleshooting#`command not found: hugo-obsidian`|the troubleshooting page]])! This will allow your terminal to correctly recognize hugo-obsidian as an executable.\n\n##  Installing Hugo\nHugo is the static site generator that powers Quartz. [Install Hugo with \"extended\" Sass/SCSS version](https://gohugo.io/getting-started/installing/) first. Then,\n\n```bash\n# Navigate to your local Quartz folder\ncd \u003clocation-of-your-local-quartz\u003e\n\n# Start local server\nmake serve\n\n# View your site in a browser at http://localhost:1313/\n```\n\n\u003e [!INFO] Docker Support\n\u003e\n\u003e If you have the Docker CLI installed already, you can avoid installing `hugo-obsidian` and `hugo`. Instead, open your terminal, navigate to your folder with Quartz and run `make docker`\n\nAfterwards, start the Hugo server as shown above and your local backlinks and interactive graph should be populated! Now, let's get it hosted online.\n\n\u003e 🌍 Step 5: [Hosting Quartz online!](notes/hosting.md)\n","lastmodified":"2024-02-27T19:19:42.831947794Z","tags":["setup"]},"/private/notes/search":{"title":"Search","content":"\nQuartz supports two modes of searching through content.\n\n## Full-text\nFull-text search is the default in Quartz. It produces results that *exactly* match the search query. This is easier to setup but usually produces lower quality matches.\n\n```yaml {title=\"data/config.yaml\"}\n# the default option\nenableSemanticSearch: false\n```\n\n## Natural Language\nNatural language search is powered by [Operand](https://beta.operand.ai/). It understands language like a person does and finds results that best match user intent. In this sense, it is closer to how Google Search works.\n\nNatural language search tends to produce higher quality results than full-text search.\n\nHere's how to set it up.\n\n1. Login or Register for a new Operand account. Click the verification link sent to your email, and you'll be redirected to the dashboard. (Note) You do not need to enter a credit card to create an account, or get started with the Operand API. The first $10 of usage each month is free. To learn more, see pricing. If you go over your free quota, we'll (politely) reach out and ask you to configure billing.\n2. Create your first index. On the dashboard, under \"Indexes\", enter the name and description of your index, and click \"Create Index\". Note down the ID of the index (obtained by clicking on the index name in the list of indexes), as you'll need it in the next step. IDs are unique to each index, and look something like `uqv1duxxbdxu`.\n3. Click into the index you've created. Under \"Index Something\", select \"SITEMAP\" from the dropdown and click \"Add Source\".\n4. For the \"Sitemap.xml URL\", put your deployed site's base URL followed by `sitemap.xml`. For example, for `quartz.jzhao.xyz`, put `https://quartz.jzhao.xyz/sitemap.xml`. Leave the URL Regex empty. \n5. Get your API key. On the dashboard, under \"API Keys\", you can manage your API keys. If you don't already have an API key, click \"Create API Key\". You'll need this for the next step.\n6. Open `data/config.yaml`. Set `enableSemanticSearch` to `true`, `operandApiKey` to your copied key, and `operandIndexId` to the ID of the index we created from earlier..\n\n```yaml {title=\"data/config.yaml\"}\n# the default option\nsearch:\n  enableSemanticSearch: true\n  operandApiKey: \"jp9k5hudse2a828z98kxd6z3payi8u90rnjf\"\n  operandIndexId: \"s0kf3bd6tldw\"\n```\n7. Push your changes to the site and wait for it to deploy.\n8. Check the Operand dashboard and wait for your site to index. Enjoy natural language search powered by Operand!\n","lastmodified":"2024-02-27T19:19:42.831947794Z","tags":[]},"/private/notes/setup":{"title":"Setup","content":"\n## Making your own Quartz\nSetting up Quartz requires a basic understanding of `git`. If you are unfamiliar, [this resource](https://resources.nwplus.io/2-beginner/how-to-git-github.html) is a great place to start!\n\n### Forking\n\u003e A fork is a copy of a repository. Forking a repository allows you to freely experiment with changes without affecting the original project.\n\nNavigate to the GitHub repository for the Quartz project:\n\n📁 [Quartz Repository](https://github.com/jackyzha0/quartz)\n\nThen, Fork the repository into your own GitHub account. **Make sure that when you fork, you _uncheck_ the 'Copy the `hugo` branch only' option**.\n\nIf you don't have an account, you can make on for free [here](https://github.com/join). More details about forking a repo can be found on [GitHub's documentation](https://docs.github.com/en/get-started/quickstart/fork-a-repo).\n\n![[notes/images/fork.png]]\n\n### Cloning\nAfter you've made a fork of the repository, you need to download the files locally onto your machine. Ensure you have `git`, then type the following command in your terminal replacing `YOUR-USERNAME` with your GitHub username.\n\n```shell\ngit clone https://github.com/YOUR-USERNAME/quartz\n```\n\n## Editing\nGreat! Now you have everything you need to start editing and growing your digital garden. If you're ready to start writing content already, check out the recommended flow for editing notes in Quartz.\n\n\u003e ✏️ Step 2: [Editing Notes in Quartz](notes/editing.md)\n\nHaving problems? Checkout our [FAQ and Troubleshooting guide](notes/troubleshooting.md).\n","lastmodified":"2024-02-27T19:19:42.831947794Z","tags":["setup"]},"/private/notes/showcase":{"title":"Showcase","content":"\nWant to see what Quartz can do? Here are some cool community gardens :)\n\n- [Quartz Documentation (this site!)](https://quartz.jzhao.xyz/)\n- [Jacky Zhao's Garden](https://jzhao.xyz/)\n- [Scaling Synthesis - A hypertext research notebook](https://scalingsynthesis.com/)\n- [AWAGMI Intern Notes](https://notes.awagmi.xyz/)\n- [Shihyu's PKM](https://shihyuho.github.io/pkm/)\n- [SlRvb's Site](https://slrvb.github.io/Site/)\n- [Course notes for Information Technology Advanced Theory](https://a2itnotes.github.io/quartz/)\n- [Brandon Boswell's Garden](https://brandonkboswell.com)\n- [Siyang's Courtyard](https://siyangsun.github.io/courtyard/)\n- [Data Dictionary 🧠](https://glossary.airbyte.com/)\n- [sspaeti.com's Second Brain](https://brain.sspaeti.com/)\n- [oldwinterの数字花园](https://garden.oldwinter.top/)\n- [SethMB Work](https://sethmb.xyz/)\n- [Abhijeet's Math Wiki](https://abhmul.github.io/quartz/Math-Wiki/)\n- [Mike's AI Garden 🤖🪴](https://mwalton.me/)\n\nIf you want to see your own on here, submit a [Pull Request adding yourself to this file](https://github.com/jackyzha0/quartz/blob/hugo/content/notes/showcase.md)!\n","lastmodified":"2024-02-27T19:19:42.831947794Z","tags":[]},"/private/notes/troubleshooting":{"title":"Troubleshooting and FAQ","content":"\nStill having trouble? Here are a list of common questions and problems people encounter when installing Quartz.\n\nWhile you're here, join our [Discord](https://discord.gg/cRFFHYye7t) :)\n\n### Does Quartz have Latex support?\nYes! See [CJK + Latex Support (测试)](notes/CJK%20+%20Latex%20Support%20(测试).md) for a brief demo.\n\n### Can I use \\\u003cObsidian Plugin\\\u003e in Quartz?\nUnless it produces direct Markdown output in the file, no. There currently is no way to bundle plugin code with Quartz.\n\nThe easiest way would be to add your own HTML partial that supports the functionality you are looking for.\n\n### My GitHub pages is just showing the README and not Quartz\nMake sure you set the source to deploy from `master` (and not `hugo`) using `/ (root)`! See more in the [hosting](/notes/hosting) guide\n\n### Some of my pages have 'January 1, 0001' as the last modified date\nThis is a problem caused by `git` treating files as case-insensitive by default and some of your posts probably have capitalized file names. You can turn this off in your Quartz by running this command.\n\n```shell\n# in the root of your Quartz (same folder as config.toml)\ngit config core.ignorecase true\n\n# or globally (not recommended)\ngit config --global core.ignorecase true\n```\n\n### Can I publish only a subset of my pages?\nYes! Quartz makes selective publishing really easy. Heres a guide on [excluding pages from being published](notes/ignore%20notes.md).\n\n### Can I host this myself and not on GitHub Pages?\nYes! All built files can be found under `/public` in the `master` branch. More details under [hosting](notes/hosting.md).\n\n### `command not found: hugo-obsidian`\nMake sure you set your `GOPATH` correctly! This will allow your terminal to correctly recognize `hugo-obsidian` as an executable.\n\n```shell\n# Add the following 2 lines to your ~/.bash_profile (~/.zshrc if you are on Mac)\nexport GOPATH=/Users/$USER/go\nexport PATH=$GOPATH/bin:$PATH\n\n# In your current terminal, to reload the session\nsource ~/.bash_profile # again, (~/.zshrc if you are on Mac)\n```\n\n### How come my notes aren't being rendered?\nYou probably forgot to include front matter in your Markdown files. You can either setup [Obsidian](notes/obsidian.md) to do this for you or you need to manually define it. More details in [the 'how to edit' guide](notes/editing.md).\n\n### My custom domain isn't working!\nWalk through the steps in [the hosting guide](notes/hosting.md) again. Make sure you wait 30 min to 1 hour for changes to take effect.\n\n### How do I setup analytics?\nQuartz by default uses [Plausible](https://plausible.io/) for analytics. \n\nIf you would prefer to use Google Analytics, you can follow this [guide in the Hugo documentation](https://gohugo.io/templates/internal/#google-analytics). \n\nAlternatively, you can also import your Google Analytics data into Plausible by [following this guide](https://plausible.io/docs/google-analytics-import).\n\n\n### How do I change the content on the home page?\nTo edit the main home page, open `/content/_index.md`.\n\n### How do I change the colours?\nYou can change the theme by editing `assets/custom.scss`. More details on customization and themeing can be found in the [customization guide](notes/config.md).\n\n### How do I add images?\nYou can put images anywhere in the `/content` folder.\n\n```markdown\nExample image (source is in content/notes/images/example.png)\n![Example Image](/content/notes/images/example.png)\n```\n\n### My Interactive Graph and Backlinks aren't up to date\nBy default, the `linkIndex.json` (which Quartz needs to generate the Interactive Graph and Backlinks) are not regenerated locally. To set that up, see the guide on [local editing](notes/editing.md)\n\n### Can I use React/Vue/some other framework?\nNot out of the box. You could probably make it work by editing `/layouts/_default/single.html` but that's not what Quartz is designed to work with. 99% of things you are trying to do with those frameworks you can accomplish perfectly fine using just vanilla HTML/CSS/JS.\n\n## Still Stuck?\nQuartz isn't perfect! If you're still having troubles, file an issue in the GitHub repo with as much information as you can reasonably provide. Alternatively, you can message me on [Twitter](https://twitter.com/_jzhao) and I'll try to get back to you as soon as I can.\n\n🐛 [Submit an Issue](https://github.com/jackyzha0/quartz/issues)\n","lastmodified":"2024-02-27T19:19:42.831947794Z","tags":[]},"/private/notes/updating":{"title":"Updating","content":"\nHaven't updated Quartz in a while and want all the cool new optimizations? On Unix/Mac systems you can run the following command for a one-line update! This command will show you a log summary of all commits since you last updated, press `q` to acknowledge this. Then, it will show you each change in turn and press `y` to accept the patch or `n` to reject it. Usually you should press `y` for most of these unless it conflicts with existing changes you've made! \n\n```shell\nmake update\n```\n\nOr, if you don't want the interactive parts and just want to force update your local garden (this assumed that you are okay with some of your personalizations been overriden!)\n\n```shell\nmake update-force\n```\n\nOr, manually checkout the changes yourself.\n\n\u003e [!warning] Warning!\n\u003e\n\u003e If you customized the files in `data/`, or anything inside `layouts/`, your customization may be overwritten!\n\u003e Make sure you have a copy of these changes if you don't want to lose them.\n\n\n```shell\n# add Quartz as a remote host\ngit remote add upstream git@github.com:jackyzha0/quartz.git\n\n# index and fetch changes\ngit fetch upstream\ngit checkout -p upstream/hugo -- layouts .github Makefile assets/js assets/styles/base.scss assets/styles/darkmode.scss config.toml data \n```\n","lastmodified":"2024-02-27T19:19:42.831947794Z","tags":[]},"/stable-diffusion":{"title":"stable diffusion","content":" \n[[2024-02-05]]\n\nIt's a technique based on [[Denoising Diffusion Probabilistic Models Notes|denoising diffusion]], where images are put into a latent [[subspace|space]] before being worked on, then the model is trained to iteratively remove random noise from images. Eventually, we can give it pure tv static and it will give a realistic image. Add in a [[CLIP guidance|CLIP]] embedding that comes from the text description of the image as a hint during training, and we can type in our own prompts along with the tv static to influence what's created. ","lastmodified":"2024-02-27T19:19:42.831947794Z","tags":[]},"/temporal-autoencoder":{"title":"temporal autoencoder","content":" #work/patientsim \n[[2024-02-06]]\n\n![[Pasted image 20240206082011.png]]\n\nVery limited. This input spectrogram was only 3s long.\n[Utilization of temporal autoencoder for semi-supervised intracranial EEG clustering and classification | Scientific Reports](https://www.nature.com/articles/s41598-023-27978-6)","lastmodified":"2024-02-27T19:19:42.831947794Z","tags":[]},"/triplet-transformer-for-patient-data":{"title":"triplet transformer for patient data","content":" #work/patientsim #paper_notes\n# Self-Supervised Transformer\n\nSeems like it's the modern, best way to do it. Triplets are stored, solving the whole sparsity problem in both the time and feature space. It's a [[transformer|transformer]], effectively getting those long-term dependencies.\nKeith's problem with the triplet idea was that the relationship between different features within the same timestep might not be so emphasized/preserved. I don't really think that'd be a problem with a transformer like this, on account of the self-attention mechanism.\n\nThe idea with a triplet is really cool. I'm biased, because I suggested it to Keith, and he found a paper that actually described just what I was talking about. Instead of having tons of empty space taken up by features that don't have data available at a given timestep, or alternatively, taken up by gaps in time where the feature doesn't have data, we can directly encode a sort of 'value' of the data, some sort of data identity, and finally the positional encoding of the data. Then, we just give one piece of data at a time. It can be lots of different kinds of features, doesn't matter, but it's one feature at a time. If they happen at the same timestamp, we just give them the same positional encoding and still put them one after another. \n![[zotero_notes/patient_sim/triplet-transformer-G7LK8EX5#triplet justification|triplet-transformer-G7LK8EX5]]\n\n\n# notes\n- is there a causal mask in the Contextual Triplet Embedding?\n- i should use the attention layer less or smth. this one uses two.","lastmodified":"2024-02-27T19:19:42.831947794Z","tags":["paper_notes","triplet"]},"/wild":{"title":"wild","content":" #work/patientsim \n2024-02-07 14:16\n\nlook at [[PatchMixer]]. Its single time-series is cut into pieces and fed into the embedding to be internally convoluted. There must be no room for [[Triplet Sparsity Reduction|triplets]]! \n\nOh, but there is. I'll devise a way to cram in, just smash together the triplet-encoded data right in ther\n\nOne curious little detail is that the paper makes a big deal out of not needing time embeddings to track anything, but with the triplets, I'll prolly need to. Otherwise, lots of features that may happen in the same timestamp will be considered to be sequential, given that the triplets only go in one at a time. Also, my data is very time-sparse, too. not continuous, like what the paper uses. So, I'll have to worry about representing gaps somehow. \n","lastmodified":"2024-02-27T19:19:42.831947794Z","tags":[]}}