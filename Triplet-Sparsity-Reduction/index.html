<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="#work/patientsim
This note was originally created in contrast to the gru-based autoencoder in the [[Merkelbach Paper]]. That&rsquo;s right, I had the idea before I found it in the paper."><meta property="og:title" content="Triplet Sparsity Reduction"><meta property="og:description" content="#work/patientsim
This note was originally created in contrast to the gru-based autoencoder in the [[Merkelbach Paper]]. That&rsquo;s right, I had the idea before I found it in the paper."><meta property="og:type" content="website"><meta property="og:image" content="https://jadenlorenc.com/icon.png"><meta property="og:url" content="https://jadenlorenc.com/Triplet-Sparsity-Reduction/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="Triplet Sparsity Reduction"><meta name=twitter:description content="#work/patientsim
This note was originally created in contrast to the gru-based autoencoder in the [[Merkelbach Paper]]. That&rsquo;s right, I had the idea before I found it in the paper."><meta name=twitter:image content="https://jadenlorenc.com/icon.png"><title>Triplet Sparsity Reduction</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://jadenlorenc.com//icon.png><link href=https://jadenlorenc.com/styles.815bbf7bb578c68d9d872162b954302d.min.css rel=stylesheet><link href=https://jadenlorenc.com/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://jadenlorenc.com/js/darkmode.dfbd211e510dfefd79efcd412978242a.min.js></script>
<script src=https://jadenlorenc.com/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://jadenlorenc.com/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://jadenlorenc.com/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://jadenlorenc.com/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://jadenlorenc.com/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://jadenlorenc.com/",fetchData=Promise.all([fetch("https://jadenlorenc.com/indices/linkIndex.96021e782b7484edbf9c531b4b4d0de8.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://jadenlorenc.com/indices/contentIndex.b0015cc6ae2d8a561f29d0a8c510a10a.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://jadenlorenc.com",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://jadenlorenc.com",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/jadenlorenc.com\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=jadenlorenc.com src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://jadenlorenc.com/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://jadenlorenc.com/>ğŸª´ jaden lorenc</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Triplet Sparsity Reduction</h1><p class=meta>Last updated
Feb 27, 2024
<a href=https://github.com/jaded0/quartz_personal_site/tree/hugo/content/Triplet%20Sparsity%20Reduction.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><ol><li><a href=#32-architecture-of-strats>3.2. Architecture of STraTS</a></li><li><a href=#32-architecture-of-strats-1>3.2. Architecture of STraTS</a></li></ol></li></ol></nav></details></aside><p>#work/patientsim</p><p>This note was originally created in contrast to the gru-based autoencoder in the <a href=/Merkelbach-Paper rel=noopener class=internal-link data-src=/Merkelbach-Paper>Merkelbach Paper</a>. That&rsquo;s right, I had the idea before I found it in the paper. I got a big head about it.</p><a href=#my-version><h1 id=my-version><span class=hanchor arialabel=Anchor># </span>My Version</h1></a><ul><li>RNN (of some sort), each data point gets a timestep of its own.</li><li>Each data point/feature gets turned into an embedding.<ul><li>basically im just saying that instead of having a big sparse representation, i&rsquo;m densifying it early. like character one-hots to word embeddings.</li><li>it also gets its own positional encoding, which may be identical to the previous iteration of the rnn bc it&rsquo;s collected at the same time.</li></ul></li><li>maybe the static data gets tacked onto the output after recurring stuff is summed, or otherwise collected into a standard vector of fixed size.</li><li>we&rsquo;re gonna have issues with long-term dependencies. That&rsquo;s just a fact.</li></ul><a href=#existing-similar-approaches><h1 id=existing-similar-approaches><span class=hanchor arialabel=Anchor># </span>existing similar approaches</h1></a><p>Another paper already does, this for use in a transformer-based approach! That&rsquo;s where I get the <a href=/triplet-transformer-for-patient-data rel=noopener class=internal-link data-src=/triplet-transformer-for-patient-data>triplet</a> name from.</p><p>It&rsquo;s super similar to the way that tokens themselves are treated in a <a class="internal-link broken">transformer</a>. The time, feature type embedding, and the value of the instance of the feature, are akin to <a class="internal-link broken">keys, queries, values</a>.</p><a href=#32-architecture-of-strats><h3 id=32-architecture-of-strats><span class=hanchor arialabel=Anchor># </span>3.2. Architecture of STraTS</h3></a><p>The architecture of STraTS is illustrated in Figure
<a href=https://ar5iv.labs.arxiv.org/html/2107.14293#S3.F3 rel=noopener>3</a>. Unlike most of the existing approaches which take a time-series matrix as input, STraTS defines its input as a set of observation triplets. Each observation triplet in the input is embedded using the Initial Triplet Embedding module. The initial triplet embeddings are then passed through a Contextual Triplet Embedding module which utilizes the Transfomer architecture to encode the context for each triplet. The Fusion Self-attention module then combines these contextual embeddings via self-attention mechanism to generate an embedding for the input time-series which is concatenated with demographics embedding and passed through a feed-forward network to make the final prediction. The notations used in the paper are summarized in Table
<a href=https://ar5iv.labs.arxiv.org/html/2107.14293#S3.T1 rel=noopener>1</a>.</p><a href=#321-initial-triplet-embedding><h4 id=321-initial-triplet-embedding><span class=hanchor arialabel=Anchor># </span>3.2.1. Initial Triplet Embedding</h4></a><p>Given an input time-series ğ“={(ti,fi,vi)}i=1n, the initial embedding for the ith triplet ğğ¢âˆˆâ„d is computed by summing the following component embeddings: (i) Feature embedding ğğ¢ğŸâˆˆâ„d, (ii) Value embedding ğğ¢ğ¯âˆˆâ„d, and (iii) Time embedding ğğ¢ğ­âˆˆâ„d. In other words, ğğ¢=ğğ¢ğŸ+ğğ¢ğ¯+ğğ¢ğ­âˆˆâ„d. Feature embeddings ğğŸ(â‹…) are obtained from a simple lookup table similar to word embeddings. Since feature values and times are continuous unlike feature names which are categorical objects, we cannot use a lookup table to embed these continuous values unless they are categorized. Some researchers (Vaswani etÂ al.,
<a href=https://ar5iv.labs.arxiv.org/html/2107.14293#bib.bib31 rel=noopener>2017</a>; Yin etÂ al.,
<a href=https://ar5iv.labs.arxiv.org/html/2107.14293#bib.bib33 rel=noopener>2020</a>) have used sinusoidal encodings to embed continuous values. We propose a novel continuous value embedding (CVE) technique using a one-to-many Feed-Forward Network (FFN) with learnable parameters i.e., ğğ¢ğ¯=FFNv(vi), and ğğ¢ğ­=FFNt(ti).</p><p>Both FFNs have one input neuron and d output neurons and a single hidden layer with âŒŠdâŒ‹ neurons and tanh(â‹…) activation. They are of the form FFN(x)=Utanh(Wx+b) where the dimensions of weights {W,b,U} can be inferred from the size of hidden and output layers of the FFN. Unlike sinusoidal encodings with fixed frequencies, this technique offers more flexibility by allowing end-to-end learning of continuous value and time embeddings without the need to categorize them.</p><p>Figure 3. The overall architecture of the proposed STraTS model. The Input Triplet Embedding module embeds each observation triplet, the Contextual Triplet Embedding module encodes contextual information for the triplets, the Fusion Self-Attention module computes time-series embedding which is concatenated with demographics embedding and passed through a dense layer to generate predictions for target and self-supervision (forecasting) tasks.</p><p><img src=https://ar5iv.labs.arxiv.org/html/2107.14293/assets/x3.png width=auto alt="Refer to caption"></p><p>Fully described in text.</p><p>Figure 3. The overall architecture of the proposed STraTS model. The Input Triplet Embedding module embeds each observation triplet, the Contextual Triplet Embedding module encodes contextual information for the triplets, the Fusion Self-Attention module computes time-series embedding which is concatenated with demographics embedding and passed through a dense layer to generate predictions for target and self-supervision (forecasting) tasks.</p><a href=#322-contextual-triplet-embedding><h4 id=322-contextual-triplet-embedding><span class=hanchor arialabel=Anchor># </span>3.2.2. Contextual Triplet Embedding</h4></a><p>The initial triplet embeddings {ğ1,â€¦,ğn} are then passed through a Transformer architecture (Vaswani etÂ al.,
<a href=https://ar5iv.labs.arxiv.org/html/2107.14293#bib.bib31 rel=noopener>2017</a>) with M blocks, each containing a Multi-Head Attention (MHA) layer with h attention heads and a FFN with one hidden layer. Each block takes n input embeddings ğ„âˆˆâ„nÃ—d and outputs the corresponding n output embeddings ğ‚âˆˆâ„nÃ—d that capture the contextual information. MHA layers use multiple attention heads to attend to information contained in different embedding projections in parallel. The computations of the MHA layer are given by</p><a href=#32-architecture-of-strats-1><h3 id=32-architecture-of-strats-1><span class=hanchor arialabel=Anchor># </span>3.2. Architecture of STraTS</h3></a><p>The architecture of STraTS is illustrated in Figure
<a href=https://ar5iv.labs.arxiv.org/html/2107.14293#S3.F3 rel=noopener>3</a>. Unlike most of the existing approaches which take a time-series matrix as input, STraTS defines its input as a set of observation triplets. Each observation triplet in the input is embedded using the Initial Triplet Embedding module. The initial triplet embeddings are then passed through a Contextual Triplet Embedding module which utilizes the Transfomer architecture to encode the context for each triplet. The Fusion Self-attention module then combines these contextual embeddings via self-attention mechanism to generate an embedding for the input time-series which is concatenated with demographics embedding and passed through a feed-forward network to make the final prediction. The notations used in the paper are summarized in Table
<a href=https://ar5iv.labs.arxiv.org/html/2107.14293#S3.T1 rel=noopener>1</a>.</p><a href=#321-initial-triplet-embedding-1><h4 id=321-initial-triplet-embedding-1><span class=hanchor arialabel=Anchor># </span>3.2.1. Initial Triplet Embedding</h4></a><p>Given an input time-series ğ“={(ti,fi,vi)}i=1n, the initial embedding for the ith triplet ğğ¢âˆˆâ„d is computed by summing the following component embeddings: (i) Feature embedding ğğ¢ğŸâˆˆâ„d, (ii) Value embedding ğğ¢ğ¯âˆˆâ„d, and (iii) Time embedding ğğ¢ğ­âˆˆâ„d. In other words, ğğ¢=ğğ¢ğŸ+ğğ¢ğ¯+ğğ¢ğ­âˆˆâ„d. Feature embeddings ğğŸ(â‹…) are obtained from a simple lookup table similar to word embeddings. Since feature values and times are continuous unlike feature names which are categorical objects, we cannot use a lookup table to embed these continuous values unless they are categorized. Some researchers (Vaswani etÂ al.,
<a href=https://ar5iv.labs.arxiv.org/html/2107.14293#bib.bib31 rel=noopener>2017</a>; Yin etÂ al.,
<a href=https://ar5iv.labs.arxiv.org/html/2107.14293#bib.bib33 rel=noopener>2020</a>) have used sinusoidal encodings to embed continuous values. We propose a novel continuous value embedding (CVE) technique using a one-to-many Feed-Forward Network (FFN) with learnable parameters i.e., ğğ¢ğ¯=FFNv(vi), and ğğ¢ğ­=FFNt(ti).</p><p>Both FFNs have one input neuron and d output neurons and a single hidden layer with âŒŠdâŒ‹ neurons and tanh(â‹…) activation. They are of the form FFN(x)=Utanh(Wx+b) where the dimensions of weights {W,b,U} can be inferred from the size of hidden and output layers of the FFN. Unlike sinusoidal encodings with fixed frequencies, this technique offers more flexibility by allowing end-to-end learning of continuous value and time embeddings without the need to categorize them.</p><p>Figure 3. The overall architecture of the proposed STraTS model. The Input Triplet Embedding module embeds each observation triplet, the Contextual Triplet Embedding module encodes contextual information for the triplets, the Fusion Self-Attention module computes time-series embedding which is concatenated with demographics embedding and passed through a dense layer to generate predictions for target and self-supervision (forecasting) tasks.</p><p><img src=https://ar5iv.labs.arxiv.org/html/2107.14293/assets/x3.png width=auto alt="Refer to caption"></p><p>Fully described in text.</p><p>Figure 3. The overall architecture of the proposed STraTS model. The Input Triplet Embedding module embeds each observation triplet, the Contextual Triplet Embedding module encodes contextual information for the triplets, the Fusion Self-Attention module computes time-series embedding which is concatenated with demographics embedding and passed through a dense layer to generate predictions for target and self-supervision (forecasting) tasks.</p><a href=#322-contextual-triplet-embedding-1><h4 id=322-contextual-triplet-embedding-1><span class=hanchor arialabel=Anchor># </span>3.2.2. Contextual Triplet Embedding</h4></a><p>The initial triplet embeddings {ğ1,â€¦,ğn} are then passed through a Transformer architecture (Vaswani etÂ al.,
<a href=https://ar5iv.labs.arxiv.org/html/2107.14293#bib.bib31 rel=noopener>2017</a>) with M blocks, each containing a Multi-Head Attention (MHA) layer with h attention heads and a FFN with one hidden layer. Each block takes n input embeddings ğ„âˆˆâ„nÃ—d and outputs the corresponding n output embeddings ğ‚âˆˆâ„nÃ—d that capture the contextual information. MHA layers use multiple attention heads to attend to information contained in different embedding projections in parallel. The computations of the MHA layer are given by</p><table><thead><tr><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>(1)</td><td></td><td>ğ‡j=softmax((ğ„ğ–jq)(ğ„ğ–jk)Td/h)(ğ„ğ–jv)j=1,â€¦,h</td><td></td></tr><tr><td>(2)</td><td></td><td>MHA(ğ„)=(ğ‡1âˆ˜â€¦âˆ˜ğ‡h)ğ–c</td><td></td></tr></tbody></table><p>Each head projects the input embeddings into query, key, and value subspaces using matrices {ğ–jq,ğ–jk,ğ–jv}âŠ‚â„dÃ—dh. The queries and keys are then used to compute the attention weights which are used to compute weighted averages of value (different from value in observation triplet) vectors. Finally, the outputs of all heads are concatenated and projected to original dimension with ğ–câˆˆâ„hdhÃ—d. The FFN layer takes the form</p><table><thead><tr><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>(3)</td><td></td><td>ğ…(ğ—)</td><td>=ReLU(ğ—ğ–1f+ğ›1f)ğ–2f+ğ›2f</td><td></td></tr></tbody></table><p>with weights ğ–ğŸğŸâˆˆâ„dÃ—2d,ğ›ğŸğŸâˆˆâ„2d,ğ–ğŸğŸâˆˆâ„2dÃ—d,ğ›ğŸğŸâˆˆâ„d. Dropout, residual connections, and layer normalization are added for every MHA and FFN layer. Also, attention dropout randomly masks out some positions in the attention matrix before the softmax computation during training. The output of each block is fed as input to the succeeding one, and the output of the last block gives the contextual triplet embeddings {ğœ1,â€¦,ğœn}.</p><a href=#323-fusion-self-attention><h4 id=323-fusion-self-attention><span class=hanchor arialabel=Anchor># </span>3.2.3. Fusion Self-attention</h4></a><p>After computing contextual embeddings using a Transformer, we fuse them using a self-attention layer to compute time-series embedding ğTâˆˆâ„d. This layer first computes attention weights {Î±1,â€¦,Î±n} by passing each contextual embedding through a FFN and computing a softmax over all the FFN outputs.</p><table><thead><tr><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>(4)</td><td></td><td>ai</td><td>=ğ®aTtanh(ğ–ağœi+ğ›a)</td><td></td></tr><tr><td>(5)</td><td></td><td>Î±i</td><td>=exp(ai)âˆ‘j=1nexp(aj)âˆ€i=1,â€¦,n</td><td></td></tr></tbody></table><p>ğ–aâˆˆâ„daÃ—d,ğ›aâˆˆâ„da,ğ®ğšâˆˆâ„da are the weights of this attention network which has da neurons in the hidden layer. The time-series embedding is then computed as</p><table><thead><tr><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>(6)</td><td></td><td>ğT=âˆ‘i=1nÎ±iğœğ¢</td><td></td></tr></tbody></table><a href=#324-demographics-embedding><h4 id=324-demographics-embedding><span class=hanchor arialabel=Anchor># </span>3.2.4. Demographics Embedding</h4></a><p>We realize that demographics can be encoded as triplets with a default value for time. However, we found that the prediction models performed better in our experiments when demographics are processed separately by passing ğ through a FFN as shown below. The demographics embedding is thus obtained as</p><table><thead><tr><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>(7)</td><td></td><td>ğd=tanh(ğ–2dtanh(ğ–1dğ+ğ›1d)+ğ›2d)âˆˆâ„d</td><td></td></tr></tbody></table><p>where the hidden layer has a dimension of 2d.</p><a href=#325-prediction-head><h4 id=325-prediction-head><span class=hanchor arialabel=Anchor># </span>3.2.5. Prediction Head</h4></a><p>The final prediction for target task is obtained by passing the concatenation of demographics and time-series embeddings through a dense layer with weights ğ°oTâˆˆâ„d, boâˆˆâ„ and sigmoid activation.</p><table><thead><tr><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>(8)</td><td></td><td>y~=sigmoid(ğ°oT[ğdâˆ˜ğT]+bo)</td><td></td></tr></tbody></table><p>The model is trained on the target task using cross-entropy loss.</p><table><thead><tr><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>(1)</td><td></td><td>ğ‡j=softmax((ğ„ğ–jq)(ğ„ğ–jk)Td/h)(ğ„ğ–jv)j=1,â€¦,h</td><td></td></tr><tr><td>(2)</td><td></td><td>MHA(ğ„)=(ğ‡1âˆ˜â€¦âˆ˜ğ‡h)ğ–c</td><td></td></tr></tbody></table><p>Each head projects the input embeddings into query, key, and value subspaces using matrices {ğ–jq,ğ–jk,ğ–jv}âŠ‚â„dÃ—dh. The queries and keys are then used to compute the attention weights which are used to compute weighted averages of value (different from value in observation triplet) vectors. Finally, the outputs of all heads are concatenated and projected to original dimension with ğ–câˆˆâ„hdhÃ—d. The FFN layer takes the form</p><table><thead><tr><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>(3)</td><td></td><td>ğ…(ğ—)</td><td>=ReLU(ğ—ğ–1f+ğ›1f)ğ–2f+ğ›2f</td><td></td></tr></tbody></table><p>with weights ğ–ğŸğŸâˆˆâ„dÃ—2d,ğ›ğŸğŸâˆˆâ„2d,ğ–ğŸğŸâˆˆâ„2dÃ—d,ğ›ğŸğŸâˆˆâ„d. Dropout, residual connections, and layer normalization are added for every MHA and FFN layer. Also, attention dropout randomly masks out some positions in the attention matrix before the softmax computation during training. The output of each block is fed as input to the succeeding one, and the output of the last block gives the contextual triplet embeddings {ğœ1,â€¦,ğœn}.</p><a href=#323-fusion-self-attention-1><h4 id=323-fusion-self-attention-1><span class=hanchor arialabel=Anchor># </span>3.2.3. Fusion Self-attention</h4></a><p>After computing contextual embeddings using a Transformer, we fuse them using a self-attention layer to compute time-series embedding ğTâˆˆâ„d. This layer first computes attention weights {Î±1,â€¦,Î±n} by passing each contextual embedding through a FFN and computing a softmax over all the FFN outputs.</p><table><thead><tr><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>(4)</td><td></td><td>ai</td><td>=ğ®aTtanh(ğ–ağœi+ğ›a)</td><td></td></tr><tr><td>(5)</td><td></td><td>Î±i</td><td>=exp(ai)âˆ‘j=1nexp(aj)âˆ€i=1,â€¦,n</td><td></td></tr></tbody></table><p>ğ–aâˆˆâ„daÃ—d,ğ›aâˆˆâ„da,ğ®ğšâˆˆâ„da are the weights of this attention network which has da neurons in the hidden layer. The time-series embedding is then computed as</p><table><thead><tr><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>(6)</td><td></td><td>ğT=âˆ‘i=1nÎ±iğœğ¢</td><td></td></tr></tbody></table><a href=#324-demographics-embedding-1><h4 id=324-demographics-embedding-1><span class=hanchor arialabel=Anchor># </span>3.2.4. Demographics Embedding</h4></a><p>We realize that demographics can be encoded as triplets with a default value for time. However, we found that the prediction models performed better in our experiments when demographics are processed separately by passing ğ through a FFN as shown below. The demographics embedding is thus obtained as</p><table><thead><tr><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>(7)</td><td></td><td>ğd=tanh(ğ–2dtanh(ğ–1dğ+ğ›1d)+ğ›2d)âˆˆâ„d</td><td></td></tr></tbody></table><p>where the hidden layer has a dimension of 2d.</p><a href=#325-prediction-head-1><h4 id=325-prediction-head-1><span class=hanchor arialabel=Anchor># </span>3.2.5. Prediction Head</h4></a><p>The final prediction for target task is obtained by passing the concatenation of demographics and time-series embeddings through a dense layer with weights ğ°oTâˆˆâ„d, boâˆˆâ„ and sigmoid activation.</p><table><thead><tr><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>(8)</td><td></td><td>y~=sigmoid(ğ°oT[ğdâˆ˜ğT]+bo)</td><td></td></tr></tbody></table><p>The model is trained on the target task using cross-entropy loss.</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/Merkelbach-Paper/ data-ctx="Triplet Sparsity Reduction" data-src=/Merkelbach-Paper class=internal-link>Merkelbach Paper</a></li><li><a href=/Transformer-based-Solutions/ data-ctx="sparsity reduction" data-src=/Transformer-based-Solutions class=internal-link>Transformer-based Solutions</a></li><li><a href=/What-sort-of-model-should-I-use/ data-ctx=triplets data-src=/What-sort-of-model-should-I-use class=internal-link>What sort of model should I use?</a></li><li><a href=/patient-similarity-model/ data-ctx=triplets data-src=/patient-similarity-model class=internal-link>patient similarity model</a></li><li><a href=/wild/ data-ctx=triplets data-src=/wild class=internal-link>wild</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://jadenlorenc.com/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Jaden Lorenc using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2024</p><ul><li><a href=https://jadenlorenc.com/>Home</a></li><li><a href=https://partizle.com/u/jaden>Lemmy</a></li><li><a href=https://github.com/jaded0>GitHub</a></li></ul></footer></div></div></body></html>