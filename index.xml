<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ðŸª´ jaden lorenc on</title><link>https://jadenlorenc.com/</link><description>Recent content in ðŸª´ jaden lorenc on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://jadenlorenc.com/index.xml" rel="self" type="application/rss+xml"/><item><title>Alt Transformer Project Description</title><link>https://jadenlorenc.com/Alt-Transformer-Project-Description/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Alt-Transformer-Project-Description/</guid><description>#project I&amp;rsquo;m a driven Data Scientist Intern, with a deep interest in machine learning and natural language processing. I recently finished a course in Deep Learning and have worked on impactful projects such as drug discovery research with a professor.</description></item><item><title>Augment WLASL</title><link>https://jadenlorenc.com/Augment-WLASL/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Augment-WLASL/</guid><description>#stablediffusion
previous attempt My little attempt at straight training an entire stable diffusion model on just the WLASL dataset was fairly ineffectual.</description></item><item><title>Candidate Weight Changes</title><link>https://jadenlorenc.com/Candidate-Weight-Changes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Candidate-Weight-Changes/</guid><description>#hebbian
[[2024-01-30]]
My loss graph looks wild:
![[/files/W&amp;amp;BChart1_30_202485501AM.png]]
Weights &amp;amp; Biases
I attribute the wave pattern to the self-reinforcing [[Hebbian Learning|weight updates]] improving performance quickly when they&amp;rsquo;ve found something right, then rapidly diverging because of biases inherent in that pattern being magnified and getting out of control.</description></item><item><title>Comparing the performance of Hebbian against backpropagation learning using conv nns</title><link>https://jadenlorenc.com/Omnivore/2023-08-04/Comparing-the-performance-of-Hebbian-against-backpropagation-learning-using-conv-nns/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Omnivore/2023-08-04/Comparing-the-performance-of-Hebbian-against-backpropagation-learning-using-conv-nns/</guid><description>Comparing the performance of Hebbian against backpropagation learning using conv nns #Omnivore #hebbian
Read on Omnivore
Read Original</description></item><item><title>Comparing the performance of Hebbian against backpropagation learning using convolutional neural ...</title><link>https://jadenlorenc.com/Omnivore/2023-08-04/Comparing-the-performance-of-Hebbian-against-backpropagation-learning-using-convolutional-neural-.../</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Omnivore/2023-08-04/Comparing-the-performance-of-Hebbian-against-backpropagation-learning-using-convolutional-neural-.../</guid><description>Comparing the performance of Hebbian against backpropagation learning using convolutional neural networks | SpringerLink #Omnivore #hebbian
Read on Omnivore Read Original</description></item><item><title>Evaluating Hebbian Learning in a Semi-supervised Setting - SpringerLink</title><link>https://jadenlorenc.com/Omnivore/2023-08-04/Evaluating-Hebbian-Learning-in-a-Semi-supervised-Setting-SpringerLink/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Omnivore/2023-08-04/Evaluating-Hebbian-Learning-in-a-Semi-supervised-Setting-SpringerLink/</guid><description>Evaluating Hebbian Learning in a Semi-supervised Setting | SpringerLink #Omnivore #hebbian
Read on Omnivore
Read Original
Highlights In this work, we propose a semi-supervised learning approach, in which the unsupervised pre-training step is performed by means of the Hebbian learning paradigm.</description></item><item><title>Hebbian Learning</title><link>https://jadenlorenc.com/Hebbian-Learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Hebbian-Learning/</guid><description>#hebbian
It&amp;rsquo;s an alternative to backprop that uses only local update rules. It seems far more similar to spiking neural networks in that it tries to follow a more &amp;ldquo;fires together, wires together&amp;rdquo; paradigm.</description></item><item><title>Visualize a Codebase</title><link>https://jadenlorenc.com/Visualize-a-Codebase/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Visualize-a-Codebase/</guid><description>#project #python
Summary: It&amp;rsquo;s hard to understand codebases. I&amp;rsquo;d like an automatic tool for it, that can show it to me in a visual structure.</description></item></channel></rss>