<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ðŸª´ jaden lorenc on</title><link>https://jadenlorenc.com/</link><description>Recent content in ðŸª´ jaden lorenc on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://jadenlorenc.com/index.xml" rel="self" type="application/rss+xml"/><item><title>Denoising Diffusion Probabilistic Models Notes</title><link>https://jadenlorenc.com/Denoising-Diffusion-Probabilistic-Models-Notes/</link><pubDate>Thu, 16 Feb 2023 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Denoising-Diffusion-Probabilistic-Models-Notes/</guid><description>#cs/advdl
![[Pasted image 20230216095306.png]] forward and backward processes are reversed in this picture. The forward proces is actually from right to left.</description></item><item><title>Alt Transformer Project Description</title><link>https://jadenlorenc.com/Alt-Transformer-Project-Description/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Alt-Transformer-Project-Description/</guid><description>#project I&amp;rsquo;m a driven Data Scientist Intern, with a deep interest in machine learning and natural language processing. I recently finished a course in Deep Learning and have worked on impactful projects such as drug discovery research with a professor.</description></item><item><title>attention</title><link>https://jadenlorenc.com/attention/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/attention/</guid><description>2024-02-07 14:38
key component of the [[transformer]]
The word embedding is fed into three layers side by side, they each make some representation of it, called query, key, and value.</description></item><item><title>Augment WLASL</title><link>https://jadenlorenc.com/Augment-WLASL/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Augment-WLASL/</guid><description>#stablediffusion
previous attempt My little attempt at straight training an entire stable diffusion model on just the WLASL dataset was fairly ineffectual.</description></item><item><title>basic</title><link>https://jadenlorenc.com/basic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/basic/</guid><description>#work/patientsim 2024-02-07 13:59
I should really go with the transformer on this one. The triplets do turn the data into a sequence of values, some with much higher relative importance than others.</description></item><item><title>Candidate Weight Changes</title><link>https://jadenlorenc.com/Candidate-Weight-Changes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Candidate-Weight-Changes/</guid><description>#hebbian #ideas
[[2024-01-30]]
My loss graph looks wild:
![[/files/W&amp;amp;BChart1_30_202485501AM.png|600]]
Weights &amp;amp; Biases
I attribute the wave pattern to the self-reinforcing [[Hebbian Learning|weight updates]] improving performance quickly when they&amp;rsquo;ve found something right, then rapidly diverging because of biases inherent in that pattern being magnified and getting out of control.</description></item><item><title>Comparing the performance of Hebbian against backpropagation learning using conv nns</title><link>https://jadenlorenc.com/Omnivore/2023-08-04/Comparing-the-performance-of-Hebbian-against-backpropagation-learning-using-conv-nns/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Omnivore/2023-08-04/Comparing-the-performance-of-Hebbian-against-backpropagation-learning-using-conv-nns/</guid><description>Comparing the performance of Hebbian against backpropagation learning using conv nns #Omnivore #hebbian
Read on Omnivore
Read Original</description></item><item><title>Comparing the performance of Hebbian against backpropagation learning using convolutional neural ...</title><link>https://jadenlorenc.com/Omnivore/2023-08-04/Comparing-the-performance-of-Hebbian-against-backpropagation-learning-using-convolutional-neural-.../</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Omnivore/2023-08-04/Comparing-the-performance-of-Hebbian-against-backpropagation-learning-using-convolutional-neural-.../</guid><description>Comparing the performance of Hebbian against backpropagation learning using convolutional neural networks | SpringerLink #Omnivore #hebbian
Read on Omnivore Read Original</description></item><item><title>COPER Paper</title><link>https://jadenlorenc.com/COPER-Paper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/COPER-Paper/</guid><description>#work/patientsim #paper_notes [[2024-02-05]]
It&amp;rsquo;s really similar to [[HybridML Paper|hybridml]], on account of the way that it pulls in mechanistic models (in this case Neural Ordinary Differential Equations).</description></item><item><title>coupled rnn paper</title><link>https://jadenlorenc.com/coupled-rnn-paper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/coupled-rnn-paper/</guid><description>#work/patientsim #paper_notes
[[2024-02-05]]
One RNN gives its output to the input of the other. Each rnn gets a different overall input.</description></item><item><title>Cyclic Plasticity Values</title><link>https://jadenlorenc.com/Cyclic-Plasticity-Values/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Cyclic-Plasticity-Values/</guid><description>#hebbian #ideas
Inspired by a youtube video by Artem Kirsanov (I think it might be this one), which describes how biological neurons have a 6hr period of varying plasticity, and each might be most plastic at a different time than the rest.</description></item><item><title>decay chat transcript</title><link>https://jadenlorenc.com/decay-chat-transcript/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/decay-chat-transcript/</guid><description>#hebbian
Me i wanna deep dive into just #2, the weight decay. Grab the relevant passage, repeat it back to me, then break it down.</description></item><item><title>deep dive into weight decay term</title><link>https://jadenlorenc.com/deep-dive-into-weight-decay-term/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/deep-dive-into-weight-decay-term/</guid><description>#hebbian
the term: ![[List of Hebbian Weight Update Rules#Weight Decay Term Added]]
The problem with [the basic Hebbian learning rule] is that there are no mechanisms to prevent the weights from growing unbounded, thus leading to possible instability.</description></item><item><title>Evaluating Hebbian Learning in a Semi-supervised Setting - SpringerLink</title><link>https://jadenlorenc.com/Omnivore/2023-08-04/Evaluating-Hebbian-Learning-in-a-Semi-supervised-Setting-SpringerLink/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Omnivore/2023-08-04/Evaluating-Hebbian-Learning-in-a-Semi-supervised-Setting-SpringerLink/</guid><description>Evaluating Hebbian Learning in a Semi-supervised Setting | SpringerLink #Omnivore #hebbian
Read on Omnivore
Read Original
Highlights In this work, we propose a semi-supervised learning approach, in which the unsupervised pre-training step is performed by means of the Hebbian learning paradigm.</description></item><item><title>Generating Synthetic Data Paper</title><link>https://jadenlorenc.com/Generating-Synthetic-Data-Paper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Generating-Synthetic-Data-Paper/</guid><description>#paper_notes #work/patientsim
Generating Synthetic Data Paper I took like 6hrs to crack this paper, reading it off and on. Since the authors were mainly interested in generating patient data, the encoding of the data into a good latent space was treated as a given.</description></item><item><title>Hebbian Learning</title><link>https://jadenlorenc.com/Hebbian-Learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Hebbian-Learning/</guid><description>#hebbian
It&amp;rsquo;s an alternative to backprop that uses only local update rules. It seems far more similar to spiking neural networks in that it tries to follow a more &amp;ldquo;fires together, wires together&amp;rdquo; paradigm.</description></item><item><title>HPCA Deets</title><link>https://jadenlorenc.com/HPCA-Deets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/HPCA-Deets/</guid><description>#hebbian #phd
[[2024-02-07]]
An extension to [[Ojas Rule]], pulling the principle of extracting the main direction from the data, out into the space for more than just one neuron at a time.</description></item><item><title>HybridML Paper</title><link>https://jadenlorenc.com/HybridML-Paper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/HybridML-Paper/</guid><description>#work/patientsim #paper_notes
[[2024-02-05]]
Not much, just a way to combine mechanistic models with ML, using easy json and TensorFlow. Bizarrely, they claim to be an open platform whilst paywalling the paper.</description></item><item><title>Informer paper</title><link>https://jadenlorenc.com/Informer-paper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Informer-paper/</guid><description>#work/patientsim #paper_notes [[2024-02-05]]
[2012.07436] Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting published in 2021
It&amp;rsquo;s just a [[transformer|transformer]] with $O(L \log{L})$ complexity in its attention mechanism, using some sparse version of self-attention via distilling it.</description></item><item><title>Layer Normalization</title><link>https://jadenlorenc.com/Layer-Normalization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Layer-Normalization/</guid><description>#hebbian
The technique stabilizes hidden state dynamics in RNNs. It may be an alternative solution to some previous issues I had with my [[Hebbian Learning|hebbian]] experiments, where the weight values, activations, and subsequently the loss numbers got increasingly erratic and large over time.</description></item><item><title>List of Hebbian Weight Update Rules</title><link>https://jadenlorenc.com/List-of-Hebbian-Weight-Update-Rules/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/List-of-Hebbian-Weight-Update-Rules/</guid><description>#hebbian
Simple Hebbian Learning Rule: $$ \Delta w_i = \eta y x_i $$
Weight Decay Term Added: $$ \Delta w_i = \eta y x_i - \gamma(\mathbf{w}, \mathbf{x}) $$</description></item><item><title>Merkelbach Paper</title><link>https://jadenlorenc.com/Merkelbach-Paper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Merkelbach-Paper/</guid><description>#work/patientsim #paper_notes
Merkelbach version Based on the MIMIC dataset.
data prep time series data collected, normalized filled in missing data, imputed via linear interpolation, or in the case of no instance of this data at all, the median static data tacked onto every single instance of the time series data GRU (it&amp;rsquo;s bidirectional) processes the whole of the data in both directions.</description></item><item><title>meta-learning with hospital data</title><link>https://jadenlorenc.com/meta-learning-with-hospital-data/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/meta-learning-with-hospital-data/</guid><description>#work/patientsim #paper_notes [[2024-02-05]]
DynEHR, written about in 2021, was frustrating to figure out, due to a paywall. I just grabbed its description from a paper that speaks about it:</description></item><item><title>model-agnostic metalearning</title><link>https://jadenlorenc.com/model-agnostic-metalearning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/model-agnostic-metalearning/</guid><description>#work/patientsim
[1703.03400] Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks
Kinda out-there. Worth further investigating. Popular enough to find stuff on youtube.</description></item><item><title>notes on patient similarity</title><link>https://jadenlorenc.com/notes-on-patient-similarity/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/notes-on-patient-similarity/</guid><description>#work/patientsim
obvious structure denoising autoencoders seem like the clear way to go for making some sort of embedding/latent space for patients the denoising is interesting for discussion, especially if we have additional boolean variables signifying whether the input is even present.</description></item><item><title>Ojas Rule</title><link>https://jadenlorenc.com/Ojas-Rule/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Ojas-Rule/</guid><description>#phd #hebbian
[[2024-02-07]]
An addition to the basic [[List of Hebbian Weight Update Rules#Simple Hebbian Learning Rule|hebbian]] learning rule that adds in the y to the subtracted term.</description></item><item><title>PatchMixer</title><link>https://jadenlorenc.com/PatchMixer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/PatchMixer/</guid><description>#work/patientsim #paper_notes [[2024-02-05]]
A convolutional neural network!! Haven&amp;rsquo;t seen one in a minute, and certainly didn&amp;rsquo;t expect it in time series processing.</description></item><item><title>patient record model type breakdown</title><link>https://jadenlorenc.com/patient-record-model-type-breakdown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/patient-record-model-type-breakdown/</guid><description>#work/patientsim
[[2024-02-06]]
transformer ![[Transformer-based Solutions]]
RNNs [[meta-learning with hospital data|DynEHR]], metalearning with [[model-agnostic metalearning|MAML]] atop an LSTM [[coupled rnn paper|coupled rnns]] well-suited to dual data types, like image+poses [[Generating Synthetic Data Paper|EHR-M-GAN]] is based on an LSTM for its encoding of time series, which is based off DRAW.</description></item><item><title>Patient Similarity Readings Log</title><link>https://jadenlorenc.com/Patient-Similarity-Readings-Log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Patient-Similarity-Readings-Log/</guid><description>#work/patientsim
1 2 3 list file.title + dateformat(file.ctime, &amp;#34; yy-MM-dd&amp;#34;) + &amp;#34; - &amp;#34; + file.tags from #work/patientsim and #paper_notes sort file.</description></item><item><title>Perceiver Paper</title><link>https://jadenlorenc.com/Perceiver-Paper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Perceiver-Paper/</guid><description>#work/patientsim [[2024-02-05]]
On the surface, it seems like it&amp;rsquo;s pretty much just: [[transformer]] for pictures and videos and images and whatnot.</description></item><item><title>Perceiver vs Informer</title><link>https://jadenlorenc.com/Perceiver-vs-Informer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Perceiver-vs-Informer/</guid><description>#work/patientsim
they&amp;rsquo;re different They were developed independently for different purposes:
The [[Perceiver Paper|Perceiver]], developed by DeepMind, focuses on generalizing the transformer architecture to handle various types of input data without task-specific modifications.</description></item><item><title>PerceiverIO Paper</title><link>https://jadenlorenc.com/PerceiverIO-Paper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/PerceiverIO-Paper/</guid><description>#work/patientsim [[2024-02-05]]
on NLP They seem to pretty much match transformers, flop-for-flop.
We first compare Perceiver IO to standard Transformers for language.</description></item><item><title>stable diffusion</title><link>https://jadenlorenc.com/stable-diffusion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/stable-diffusion/</guid><description>[[2024-02-05]]
It&amp;rsquo;s a technique based on [[Denoising Diffusion Probabilistic Models Notes|denoising diffusion]], where images are put into a latent [[subspace|space]] before being worked on, then the model is trained to iteratively remove random noise from images.</description></item><item><title>temporal autoencoder</title><link>https://jadenlorenc.com/temporal-autoencoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/temporal-autoencoder/</guid><description>#work/patientsim [[2024-02-06]]
![[Pasted image 20240206082011.png]]
Very limited. This input spectrogram was only 3s long. Utilization of temporal autoencoder for semi-supervised intracranial EEG clustering and classification | Scientific Reports</description></item><item><title>Transformer-based Solutions</title><link>https://jadenlorenc.com/Transformer-based-Solutions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Transformer-based-Solutions/</guid><description>#work/patientsim
[[COPER Paper]] just used perceivers, and physics models we could use [[Informer paper|informers]], which effectively lengthen the context length, and are especially good at time-series.</description></item><item><title>Triplet Sparsity Reduction</title><link>https://jadenlorenc.com/Triplet-Sparsity-Reduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Triplet-Sparsity-Reduction/</guid><description>#work/patientsim
This note was originally created in contrast to the gru-based autoencoder in the [[Merkelbach Paper]]. That&amp;rsquo;s right, I had the idea before I found it in the paper.</description></item><item><title>triplet transformer for patient data</title><link>https://jadenlorenc.com/triplet-transformer-for-patient-data/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/triplet-transformer-for-patient-data/</guid><description>#work/patientsim #paper_notes
Self-Supervised Transformer Seems like it&amp;rsquo;s the modern, best way to do it. Triplets are stored, solving the whole sparsity problem in both the time and feature space.</description></item><item><title>Visualize a Codebase</title><link>https://jadenlorenc.com/Visualize-a-Codebase/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/Visualize-a-Codebase/</guid><description>#project #python
Summary: It&amp;rsquo;s hard to understand codebases. I&amp;rsquo;d like an automatic tool for it, that can show it to me in a visual structure.</description></item><item><title>What sort of model should I use?</title><link>https://jadenlorenc.com/What-sort-of-model-should-I-use/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/What-sort-of-model-should-I-use/</guid><description>#work/patientsim
[[2024-02-07]]
Well, there&amp;rsquo;s lots of [[patient record model type breakdown|examples]]. I&amp;rsquo;m pretty sold on those [[Triplet Sparsity Reduction|triplets]] because they&amp;rsquo;re cool and they solve the whole time+feature space sparsity issue in a pretty way.</description></item><item><title>wild</title><link>https://jadenlorenc.com/wild/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jadenlorenc.com/wild/</guid><description>#work/patientsim 2024-02-07 14:16
look at [[PatchMixer]]. Its single time-series is cut into pieces and fed into the embedding to be internally convoluted.</description></item></channel></rss>